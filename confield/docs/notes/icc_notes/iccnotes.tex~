\documentclass[12pt]{article}
%\usepackage{bookman}
\usepackage{/home/sci/weiliu/haldefs}
\usepackage{graphicx}
\usepackage{url}
\usepackage{textcomp}
\usepackage{enumitem}
\usepackage{subfig}
\usepackage{hyperref}
%\usepackage{/home/sci/weiliu/packages/breakurl/breakurl}
\usepackage{amsmath}
\usepackage{verbatim}
\usepackage{natbib}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{color}
\usepackage{mdwlist}

\hypersetup{
  % bookmarks=true,         % show bookmarks bar?
    unicode=false,          % non-Latin characters in Acrobat’s bookmarks
    pdftoolbar=true,        % show Acrobat’s toolbar?
    pdfmenubar=true,        % show Acrobat’s menu?
    pdffitwindow=false,     % window fit to page when opened
    pdfstartview={FitH},    % fits the width of the page to the window
    pdftitle={Reading notes},
    pdfauthor={Wei Liu},     % author
    pdfsubject={Reading notes},   % subject of the document
    pdfcreator={Wei Liu},   % creator of the document
    pdfproducer={Producer}, % producer of the document
    pdfkeywords={reading, papers, functional MRI, connectivity}, % list of keywords
    pdfnewwindow=true,      % links in new window
    colorlinks= true,       % false: boxed links; true: colored links
    linkcolor=red,          % color of internal links
    citecolor=blue,        % color of links to bibliography
    filecolor=magenta,      % color of file links
    urlcolor=cyan           % color of external links
}


\setlength{\oddsidemargin}{0 in}
\setlength{\evensidemargin}{0 in}
\setlength{\topmargin}{-0.6 in}
\setlength{\textwidth}{6.5 in}
\setlength{\textheight}{9 in}
\setlength{\headsep}{0.5 in}
\setlength{\parindent}{0 in}
\setlength{\parskip}{0.1 in}



\begin{document}
\title{Reading notes}
\author{Wei Liu}
\maketitle
%% \tableofcontents

This is a summary of the readings of the recent functional connectivity papers. 

\textbf{dynamic connectivity: } It looks like people are begining to study the
dynamic property of the connectivity \citep{deco2010emerging}. The conventional
brain connectivity analysis assume that the connections between regions are
static and do not change with time. \cite{zalesky2011relationship} test the
variation of the fMRI time series by splitting the time series into two part,
and use paired t-test to test any difference between two halves with respect to
the various statistical measures, including variance, maximum and minimum
amplitude, and did not identify significant difference. This suggest there is no
long-term variation on the time series. But this is not sufficient to suggest
that the functional network is static. The connectivity between any regions can
change over time. Modeling the dynamics of the connectivity will help to
understand the brain network. If we use graph to model brain network, we can use
the rich theory in the dynamics of the graph \citep{mortveit2008introduction} to
model the dynamics of the brain network. \cite{deco2010emerging} reviewed three
typical methods for modeling the network dynamics.


\section{Sparsity of Brain Network}
Theare are a few reasons that we should explore the sparse concept of graphical model to fMRI.
\begin{itemize}
  \item \textbf{Consistence with neurophysiology} The coefficients computed the regression with sparcity method corresponds to the partial correlation after removing the influence of all other nodes in the graph. This amounts to the direct connection between two regions. The regular correlation matrix have a marginal interpretation. The inverse covariance matrix of the multi-variate Gaussian gives a \emph{conditional} interpretation. The zeros in the precision matrix means conditional independence given all other nodes, and the coefficients means partial covariance (or partial correlation, after normalization of the the data).

    Any two regions of the human may have some degree of correlation marginally, due to the possible connections to other nodes. The partial correlation answer the questions like what is the direct link between two regions. This can also be explained with diffusion tensor imaging where fiber tracts either directly or indirectly connect two white matter regions.

  \item \textbf{Rich method for optimization}
  \item \textbf{Network analysis as postprocessing}
  \item \textbf{Possibility to parallelize} The graphical Lasso problem can be seen as $P$ independent linear regression problem, which can be parallelized.
\end{itemize}

\section{April 2 - 9}
The functional network is said to be orgainized in a hierarichical way. We can explore this. 

Markov Random field  can be used in functional connectivity of fMRI data and represent the statistical dependency of spatial adjacent variables. The representation can be either in voxel level or in a higher level where objects or regions of interest are variables. The topology or structure of the graph used by MRFs can be known as regular lattice, or it can be learned from the data. The model as a prior information, helps the statistical inference of the connectivity. 

\section{April 9 - 16}
Need to test synthetic data for parameter estimation. Find a scenario where the parameters can be decently estimated. We need a good validation method for this hierarchical MRF method.

Also need to revise the thesis claim. And for the dynamical network, draft a summary of the readings. 

\citet{honey2007network} studies the dynamics of functional network on a single subject. It is helpful if we can build a model that can use information from multiple subjects. in \cite{honey2007network}, the structure connectivity matrix is first computed, and neural dynamical potential signal is \emph{simulated} by some models. The neural potential has high sampling rate. The functional connectivity is computed from this simulated time course signal at different temporal scale. BOLD signal is also \emph{simulated} from the neural potential signal by the \emph{balloon} model. It seems the simulated BOLD signal has course spatial resolution compared the neural potential signal, i.e., BOLD is simulated per region. The conclusion is that functional connectivity estimated from the whole neural potential signals are more consistent with structural network, while the network estimated from a segment of neural signals have more transient network patterns. 

My comments: For the study of fMRI, we do not have to assume a model for neural potential signal and simulate it. We can just assume a dynamic for BOLD signal. The model should take account multiple subjects. We should model the common property among all the subjects. For example, although different subjects may not in the same state at same time point on same ROI, they may share probability of transiting from one state to the next. Subjects must share something. 

To model the dynamics of the network, we can either explicitly model the time delay due to the transmission of the neural signal in the fibers, or we can choose not model this time delay. \cite{honey2007network} does not model it. We probably should not model it either, because the low temporal resolution of BOLD signal. The time delay, compared to the TR of BOLD, can be ignored. 

\cite{ghosh2008noise} is the second paper that \citet{deco2010emerging} reviewed. In \citeauthor{ghosh2008noise}, the low level neural singal is also simulated similar to \citeauthor{honey2007network}, and BOLD signal is computed from these simulated signals. The difference from \citeauthor{honey2007network} is they add time delay between the regions, and the delay is proportional to the distance between the regions.

If we have a dynamical model for BOLD signal, how could we simulated it to see the simulated BOLD signal look like the real data? Dynamical Gaussian Markov Random Field??? 

In \cite{yeo2011organization}, 7 and 17 functional network is estimated by clustering approach. This functioal atlas is used in a review of ADHD \citep{castellanos2011large}. From \citeauthor{castellanos2011large}, DMN is has less anti-correlation for ADHD patients, since the strong anti-correlation is believed to related to the better behavioral performance. 

Another interesting thing to do is building a spatial-temporal model of fMRI data. The model will depict the spatial dependency, as well as the temporal correlation of the BOLD signal. This model can be used to estimate and remove noise. Such a model is related to the MRF because spatially it would be a MRF, and temporal domain the image at time point $t$ will depend on the image at $t-1$. 

\section{April 23 - 30}
What we have been done so far is far from being a jounarl paper. 

The advantage of variational inference: 1) Able to estimate number of components K. 2) Use indicator vector notation instead of scalar variable for hidden variables. 3) Introduce Dirichlet distribution for the mixing coefficients pi. 4) Less memory usage. 5) Less running time. 6) Possible for full Bayesian analysis.

The advantage of using \textsf{lemon} graph library for rewriting the code: 1) Graph structure separate from data. 2) Easier to change graph structure without chaging other code. 3) Ease to change MRF neighborhood structure, say, from 6 neighbors to 26 neighbors.

The Dirichlet prior on mixing coefficient Pi is different from the MRF on hidden variable z.

\section{April 30 - May 6}
\subsection{Consistency test}
This test aims to see the consistency of the joint log likelihood function $p(x,y)$ over the number of MC samples. 
\begin{figure}
  \includegraphics[width=0.45\textwidth]{figures/n10}
  \includegraphics[width=0.45\textwidth]{figures/n50}\\
  \includegraphics[width=0.45\textwidth]{figures/n10_zoom}
  \includegraphics[width=0.45\textwidth]{figures/n50_zoom}
\end{figure}
\subsection{Swendsen-Wang}
First I will explain how Swendsen-Wang sampling works on a simple Ising or Potts model. Define the Potts mode as
\begin{align*}
  f(x) &= \frac{1}{Z}\exp \{H(x)\}, \quad H(x) = \sum_{i<j} \beta \Ind_{(x_i = x_j)}.
\end{align*}
This is a slightly different definition with our previous definition of Potts model, but they both prefer smoothed labels between spatial neighbors when $\beta > 0$. Now to generate a sample from $f(x)$, we introduce a auxiliary random variable $y_{ij}, i \le j$. Conditioned on $x$, the $y_{ij}$ are independent. Each $y_{ij}$ is uniformly distribution on the interval $[0, a_{ij}]$, with $a_{ij} = \exp(\beta\Ind_{(x_i = x_j} ) \leq 1$. So the conditional pdf of $y = \{y_{ij}\}$ given $x$ is
\begin{equation*}
  f(y|x) = \prod_{i\le j}\frac{\Ind_{(y_{ij} \leq a_{ij})}}{a_{ij}} = \left (\prod_{i\le j}\Ind_{y_{ij} \leq a_{ij}} \right )\exp \{-H(x)\}
\end{equation*}
Due to this definition of $f(y|x)$, the joint pdf $p(x,y)$ is also uniformly distributed because of the cancellation of $H(x)$ and $-H(x)$.
\begin{align*}
  f(x,y) &= f(x) f(y|x) = \frac{1}{Z}\exp\{H(x)\} \cdot  \left (\prod_{i\le j}\Ind_{y_{ij} \leq a_{ij}} \right )\exp \{-H(x)\} \\
  & =  \frac{1}{Z} \prod_{i\le j}\Ind_{y_{ij} \leq a_{ij}} = \left \{
  \begin{array}{l l}
    \frac{1}{Z} & \quad \text{if $y_{ij} \leq a_{ij}, \forall i < j,$}\\
    0 & \quad \text{otherwise}\\
    \end{array} \right.
\end{align*}
More importantly, $p(x|Y) \prop f(x,y)$ is also uniformly distributed over the set $\cA = \{x: y_{ij} \leq a_{ij}\}$. Now either $y_{ij} \in [0, 1]$ or $y_{ij} \in (1, e^\beta]$. if $y_{ij} \leq 1$, it is impossible to tell if $x_i = x_j$. But if $y_{ij} > 1$, $a_{ij} $ must be greater than 1 to allow draw a uniform sample $y_{ij} > 1$ from $f(y_{ij} | x)$, and hence $x_i = x_j$. Therefore, The sites $i$ and $j$ for which $y_{ij} . 1$ can be gathered into clusters, and within each such cluster the $x$ must be same. And the value of $x$ is uniformly distributed. The $x_i$  and $x_j$ value for those $y_{ij} \leq 1$ are not constrained to be same and can be different.

Hence we can sample from $f(y|x)$ and $f(x|y)$. At last we discard $y$ and the $x$ will be samples from marginal distribution $f(x)$. To simplify matters further, note that instead of the exactly $y_{ij}$ it suffices to know only the variables $u_{ij} = \Ind_{y_{ij} \leq 1}$. Given $x$, $u_{ij}\sim \Ber(1 - e^{-\beta})$ if $x_i = x_j$, and $u_{ij} = 0$ otherwise. Hence we have the Swendsen-Wang algorithm as: 1) Given $x_i$, generate $u_{ij} \sim \Ber(1 - e^{-\beta})$ if $x_i = x_j$, and $u_{ij} = 0$ otherwise. 2) Given $\{u_{ij}\}$, generate $x$ by clustering all the sites and choosing a label value for each cluster independently and uniformly from $\{1,\dots, K\}$.


\subsection{Generalized Swendsen-Wang}
The generalized Swendsen-Wang by \cite{barbu2005generalizing} has a few things to note:
\begin{itemize}
\item It choose only one cluster to change label $x$ in each iteration, instead of sample all clusters.
\item It seems the posterior distribution used in their sampling only includes likelihood term (i.e. the data term), and not includes the smoothness prior. This need special attention. 
\end{itemize}

\section{May 14 -- May 21}
Since he SW algorithm takes longer time than regular Gibbs sampler, I used a profiler to look into the call trace of valgrind tool, and found that: When sampling on a single larger image (128x128x128, which amounts to 10 subjects data of real fMRI), about half of the running time is spent on finding connect component. Around 25\% time is used for generating random samples. About 11\% is spent on the other operations on the graph such as access nodes and edges. 

If we want to re-write the code in parallel, at most 50\% of the routine can be optimized, and the connect component routine can not be parallelized easily, since it is provided by the \textsf{lemon} library. We do need this library in order to define a data structure for both graph nodes and edges. Even if we do not use this library, and write our own connected component routine, it is not straightforward how to optimize it in parallel, since the algorithm is implmented with a breath-first-search algorithm.

Other issues: Critical temperature,  variance map.

\section{Aug 24 -- Aug 27}
I'm currently working on two things: 1) I think we may need a change on the
hierarchical model. We don't really need the MRF in group level, since the
subjects MRF can enforce the smoothness of group. There are advantages of this
simplification: a) we can define 1-of-K binary vector on subjects level, and
define a continuous variable on group. So the group naturally acts as a prior
probability, just as we tried to do some time ago. b) It would be possible to
define a Dirichlet prior on group, and we can probably even estimate the number
of clusters K. 2) If the sampling algorithm's issue is too difficult to solve,
there is no reason we can not try other optimization methods like Variational
Inference. The benefits are: a) Full Bayesian. b) faster. The solution would
still be a local optima, though. There are rich theory along this path, so we
don't worry about the lack of good methodology/theory.

OK this is not the right thing to do for now. 

Wednesday: Spent some time on MTGL graph library. Though the computation time is
not a issue if the algorithm works fine, it is an issue when in debugging
stage. The MTGL library is claim to have parallel algorithm than can use
multiple threads. After looking into it, I found this is not what I want. I just
need use the Lemon graph library and update each node with my own pthread
routine. I don't really need a parallel a parallel graph library. MTGL's
parallel implementation is on some specific algorithm, which I don't need except
in a Swendsen-Wang algorithm for connected component analysis.

So if I do need a graph library, Lemon is the choice so far. But to migrate the
current code to the Lemon graph is low priority compared to testing on new
datasets.

Thursday and Friday: Downloaded NYU's test-retest dataset and working on the
preprocessing scripts. The plan is to use fcon 1000's scripts which uses FSL and
AFNI tool. This scripts is similar to ADHD200's scripts but is more geenral.



%% \section{backup}
%% To see the difference between sparse graphical model and thresholding on correlation matrix, we can remove a link from a full connected graph, and generate data based on this graph. The correlation matrix, after thresholded, should be able to give a binary graph with one link missing. 

%% The graph matching method can be used to measure the distance between two graphs, therefore used for compare functional networks.

%% Also freesurfer to unfold the cortex in future test.

%% Use K-means output as a initial nodes for functional connectivity analysis (sparsity etc)

%% We do not have to use hierarchical model for inverse covariance estimation. We can estimate the network and compare them by graph method.
\bibliographystyle{plainnat}
\bibliography{/home/sci/weiliu/projects/centralref}
\end{document}
