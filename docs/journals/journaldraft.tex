\documentclass{article}
\usepackage{amssymb}
\usepackage{amsfonts}

\renewcommand{\vec}[1]{\boldsymbol{#1}}
\newcommand{\mat}[1]{\text{\bf #1}}

\author{Bo Wang, Wei Liu, etc}

\title{Active cut based 4D pathological anatomy modeling:
 A TBI imaging study}

\begin{document}
\maketitle
\begin{abstract}
\end{abstract}

\section{Introduction}
\section{Methods}
\subsection{Graph cuts}
% need to make notes about our method's different with reuglar graph cuts (or
% GrabCut) method. We changed the difiniotn of the data term of the graph cut's
% objective function. Instead using the regular Gaussian mixture model's
% marginal defintion, we use the expectation of the joint log-likelihood with
% respet to the hidden variables. The benefit of this swith is, now the GMM and
% graph cuts optimize a single unified objective function. Previous GrabCut does
% not have this property. (although it does optimize the single marginal
% likelihood. Here I need to double check the original GrabCut paper). 

% Also need to change the defintion of the posterior predictive proabbility as a
% query score. If we just use the smoothness term of the graph cuts to define
% the MRF prior of the predictive probability, the model of the predictive
% probability will be same as the model of graph cuts, which is a good
% thing. However, now the prior also includes the data term, and this will
% violate the Bayesian rule.

% we probably need to talk about different parameter settings and its impact on
% the results.

% we may try to do the segmentation even without the user's initial bounding
% box. Is it possible to look at the FLAIR and other channels to find the
% initial large lesion region? Guess we will end up finding many false-positive
% voxels at CSF. How to deal with it?


\noindent \textbf{Graph Cuts global optimization: }We define the objective
function of graph cuts optimization as
% possible to move two energy term in to text. Just keep E(alpha) here.
\begin{align*}
  &\mat E(\alpha) = \gamma \sum_{(m,n)} \psi(\alpha_n, \alpha_m) \exp \left ( -\beta_0 \| \vec x_n - \vec x_m\|^2 \right ) +\\
  & \sum_n^N \mathbb{E}_{p(\mat z|\mat x)} \log p(\vec x_n, \vec z_n; \theta (\alpha)),
  &\mathbb{E}_{p(\mat z|\mat x)} \log p(\vec x, \vec z; \theta (\alpha_n)) = \!\!\sum_{k=1}^K \!z_{nk}(\log \pi_{nk} \!+\! \log \mathcal{N}(\vec x_n; \theta(\alpha_n))) \!+\!  \beta \!\!\!\!\!\!\sum_{m \in \mathcal{N}(n)} \!\!\!\!\!\langle \vec z_n, \!\vec z_m\rangle \!-\! \log C \!\!\!\!,
\end{align*}
where $\psi = 1$ if $\alpha_n \neq \alpha_m$, otherwise $\psi = 0$, and
$\beta_0$ is estimated from data by taking expectation over image
sample~\cite{boykov2001fast,rother2004grabcut}. The smoothness constraint
depends on the data term, so it is a conditional random field. When building the
graph, we set the T-link by the second term, and the N-link by the first term in
the above equation.



\section{Experiments}
\end{document}
