\documentclass[a4paper]{llncs}


\usepackage{amssymb}
\setcounter{tocdepth}{3}
\usepackage{graphicx}

\usepackage{url}
\newcommand{\keywords}[1]{\par\addvspace\baselineskip
  \noindent\keywordname\enspace\ignorespaces#1}

%%%%%% Added by Wei Liu %%%%%%%
\usepackage{amsmath}
\usepackage[vlined,ruled]{algorithm2e}
\usepackage{lwdefs}
\usepackage{multirow}
%%%%%% End of Added by Wei Liu %%%%%%%


\begin{document}

\mainmatter  % start of an individual contribution

\title{Monte Carlo Expectation Maximization with Hidden Markov Models to Detect Functional Networks in Resting-State fMRI}

\maketitle


\begin{abstract}
We propose a novel Bayesian framework for partitioning the cortex into
distinct functional networks based on resting-state fMRI. Spatial
coherence within the network clusters is modeled using a hidden Markov
random field prior. The normalized time series data, which lie on a
high-dimensional sphere, are modeled with a mixture of von
Mises-Fisher distributions. To estimate the parameters of this model,
we maximize the posterior using a Monte Carlo Expectation Maximization
(MCEM) algorithm in which the intractable expectation over all
possible labelings is approximated using Monte Carlo integration. We
show that MCEM solutions on synthetic data are superior to those
computed using a mode approximation of the expectation step. Finally,
we demonstrate on real fMRI data that our method is able to identify
visual, motion, attention and default mode networks with considerable
consistency between subjects.
\end{abstract}

\section{Introduction}
Resting-state functional MRI (fMRI) measures the background fluctuations in the
blood oxygen level-dependent signal of the brain at rest. The temporal
correlations between these signals are used to estimate the functional
connectivity of different brain regions. This technique has shown promise as a
clinical research tool to describe functional abnormalities, including
applications in Alzheimer's disease, schizophrenia, and autism
\cite{fox2010clinical}. In resting-state fMRI, a standard analysis procedure is
to select a region of interest (ROI), or seed region, and find the correlation
between the average signal of the ROI and other voxels in the brain. These
correlations are thresholded so that only those voxels with significant
correlations to the seed region are shown. Recent methods to find functional
networks without seed regions include Independent Component Analysis
(ICA)~\cite{beckmann2005tensorial}, which often includes an ad hoc method to
manually choose those components that are anatomically meaningful. Other
approaches employ clustering techniques to automatically partition the brain
into functional networks. An similarity metric is defined, e.g., by correlation
\cite{5074650} or by spectral coherence \cite{thirion2006detection}, and then a
clustering method such as $k$-means or spectral clustering are used to group
voxels with similar time series. A drawback of these approaches is that they
disregard the spatial position of voxels, and thus ignore the fact that
functional networks are organized into sets of spatially coherent regions.

We introduce a new data-driven method to partition the brain into networks of
functionally-related regions from resting-state fMRI. The proposed
algorithm does not require specification of a seed region, and there is no ad
hoc thresholding or parameter selection. We make a natural assumption that
functionally homogeneous regions should be spatially coherent. Our method
incorporates spatial information through a Markov Random Field (MRF) prior on
the voxel labels, which models the tendency of spatially-nearby voxels to be
within the same functional network. Each time series is first normalized to zero
mean and unit norm, which results in data lying on a high-dimensional unit
sphere. We then model the normalized time-series data as a mixture of von
Mises-Fisher (vMF) distributions \cite{banerjee2006clustering}. Each component
of the mixture model corresponds to the distribution of voxel time series from
one functional network. Solving for the parameters in this combinatorial model
is intractable, and we therefore use a stochastic method called Monte Carlo
Expectation Maximization (MCEM), which approximates the expectation step using
Monte Carlo integration. The stochastic property of MCEM make it possible to
explore a large solution space, and it performs better than the standard mode
approximation method like iterated conditional modes (ICM).

The proposed method in this paper is related to previous approaches using MRFs
to model spatial relationships in fMRI data. Descombes, et
al.~\cite{descombes_spatio-temporal_1998} use a spatio-temporal MRF to analyze
task activation fMRI data. Liu, et al.~\cite{liu2010spatial} use a MRF model of
resting state fMRI to estimate pairwise voxel connections. However, neither of
these approaches tackle the problem of clustering resting state fMRI into
functional networks.

\section {Hidden Markov Models of Functional Networks}\label{sec:models}
% Bayesian, prior, likelihood
We use a Bayesian statistical framework to identify functionally-connected
regions of the gray matter in fMRI data. We formulate a generative model, which
first generates a configuration of functionally-connected regions in the brain,
followed by an fMRI time series for each voxel based on its region. We employ an
MRF prior to model region configurations, represented via unknown, or
\emph{hidden}, labels. Given a region, we assume that the fMRI time series,
normalized to zero mean and unit norm, are drawn from a von Mises-Fisher
likelihood.

% notation: indices, labels

Let $\cS = \{1, \ldots, N\}$ be the set of indices for all gray-matter
voxels. We assume that the number of regions $L$ is a known free
parameter. Let $\cL = \{1, 2, \cdots, L\}$ be the set of labels, one
for each region. We denote a label map for functionally-connected
regions as a vector $\mat z = (z_1,\dots, z_N), z_i \in \cL$.
%Let $\cZ = \cL^N$ be the set of all possible $\mat z$ configurations.

\subsection{Markov Prior Model}
Physiologically, the label map $\mat z$ should be piecewise
smooth. Thus, we use the \emph{Potts} MRF~\cite{LiBook} to model $\mat
z$:
\begin{equation*}
  P(\mat z)
  =
  \frac
  {1}
  {C}
  \exp
  \Bigg(\!-\beta
  \sum_i
  \sum_{j \in \cN_i}
  T (z_i \neq z_j)
  \Bigg),
\end{equation*}
where $T$ is $1$ if its argument is true and $0$ otherwise; $\cN_{i}$
is the set of neighbors of $i$ as defined by the neighborhood system
underlying the MRF; $\beta > 0$ is a model parameter controlling
label-map smoothness; $C$ is a normalization constant that is the sum
of $P(\mat z)$ over all possible configuration of $\mat z$. The
Markov-Gibbs equivalence~\cite{LiBook} implies that the conditional
distribution of $z_i$ at site $i$ is:
\begin{equation}
  \label{eq:mrfprior}
  P(z_i | \mat z_{-i})
  =
  P(z_i | z_{\cN_i})
  =
  \frac
  {
    \exp \left( -\beta \sum_{j \in \cN_i} T(z_i \neq z_j) \right)
  }
  {
    \sum_{l \in \cL} \exp \left( -\beta \sum_{j \in \cN_i}T(l \neq z_j) \right)
  },
\end{equation}
where $\mat z_{-i}$ is the collection of all variables in $\mat z$
excluding site $z_i$.


\subsection {Likelihood Model}


% normalization

To identify functionally homogeneous regions, the time series at each voxel are
normalized to zero mean and unit norm. After
normalization, the sample correlation between two time series is equal to their
inner product on the sphere. Thus, to find regions with high correlated voxels
is equivalent to find regions in which the normalized time series have larger
inner products. On the sphere, two data points with small shortest path will
have large inner product, and hence have large Pearson correlation in original
space. So we re-formulate the problem of finding clusters of voxels with higher
correlations, to the problem of finding clusters on sphere that has small
shortest path, or big inner products.

% likelihood

We use the notation $\mat x = \{ (\vec x_1, \dots, \vec x_N)\, |\,
\vec x_i \in S^{p-1} \}$ to denote the set of \emph{normalized} time
series. Observe that given $\mat z \in \cZ$, the random vectors $\vec
x_i$ are conditional independent. Thus, the likelihood $\log
P(\mat x | \mat z) = \sum_{i \in \cS} \log P (\vec x_i | z_i)$. We
model the emission function $P(\vec x_i | z_i)$ using the von
Mises-Fisher distribution:
\begin{equation}
  f (\vec x_i;\vec \mu_l, \kappa_l | z_i = l)
  =
  C_p(\kappa_l)
  \exp
  \left(\kappa_l \vec \mu_l^T \vec x_i\right),
  \quad
  \vec x_i \in S^{p-1},
  \quad
  l \in {\cL}
  \label{eq:vmf}
\end{equation}
where, for the cluster labeled $l$, $\vec \mu_l$ is the mean
direction, $\kappa_l \geq 0$ is the \emph{concentration parameter},
and the normalization constant $C_p(\kappa) = {\kappa^{\frac{p}{2} -
    1}}/\{{(2\pi)^{\frac{p}{2}} I_{\frac{p}{2}-1}(\kappa)}\}$, where
$I_\nu$ denotes the modified Bessel function of the first kind with
order $\nu$. The larger the $\kappa$, the greater is the density
concentrated around the mean direction. Since \eqref{eq:vmf} depends
on $\vec x$ only by $\vec \mu^T \vec x$, the vMF distribution is
unimodal and rotationally symmetric around $\vec \mu$.

% 'prior' on kappa

In the Bayesian framework, we also define distributions on
parameters. We assume that $\forall l \in \cL$, $\kappa_l \sim
\cN(\mu_{\kappa}, \sigma_{\kappa}^2)$ with hyperparameters
$\mu_{\kappa}$ and $\sigma_{\kappa}^2$ that can be set
empirically. This prior enforces constraints that the clusters should
not have extremely high or low concentration parameters. We carefully
choose hyperparameter values to ensure that the results are not very
sensitive to the hyperparameter values.


\section{Monte Carlo EM}
\label{sec:mcem}


To estimate the model parameters and the hidden labels, we use a
stochastic variant of expectation maximization (EM) called Monte Carlo
EM (MCEM)~\cite{wei1990monte}. The standard EM algorithm maximizes the
expectation of the log-likelihood of joint pdf of $\mat x$ and the
hidden variable $\mat z$ with respect to the posterior probability
$P(\mat z | \mat x)$, i.e. $\mathbb{E}_{P(\mat z| \mat x)} [\log
  P(\mat x, \mat z; \vec \theta)]$. The combinatorial number of
configurations for $\mat z$ makes this expectation intractable. Thus, we
use Monte Carlo simulation to approximate this expectation as
\begin{equation}
  \widetilde Q(\vec \theta; \mat x, \mat z)
  \approx
  \frac
  {1}
  {M}
  \sum_{m=1}^{M}
    \log
    P (\mat z^m; \beta)
    +
    \log
    P (\mat x | \mat z^m; \vec \theta_L),
  \label{eq:mcemq}
\end{equation}
where $\mat z^m$ is a sample from $P(\mat z | \mat x)$, $\vec \theta_L = \{\vec
\mu_l, \kappa_l : l \in \cL\}$ is the parameter vector of the likelihood, and
$\vec \theta=\{\beta, \vec \theta_L\}$ is the full parameter vector of the
model. Computing the MRF prior in \eqref{eq:mcemq} is still intractable due to
the normalization constant, and we instead use a pseudo-likelihood
approximation~\cite{LiBook}, which gives
\begin{align*}
  \widetilde Q
  &
  \approx
  \frac{1}{M}
  \sum_{m=1}^{M}
  \sum_{i \in \cS}
  \log P(z_i | z_{\cN_i}; \beta)
  +
  \frac{1}{M}
  \sum_{m=1}^{M}
  \sum_{i \in \cS}
  \log
  P(\vec x_i | z_i; \vec \theta_L)
  =
  \widetilde Q_P
  +
  \widetilde Q_L.
\end{align*}
We use $\widetilde Q_P$ to denote the log-pseudo-likelihood of the prior
distribution, and use $\widetilde Q_L$ to denote the log-likelihood
distribution.


\subsection{Sampling from the Posterior}


Given the observed data $\mat x$ and parameter value $\vec \theta =
\{\beta, \vec \theta_L \}$, we sample from the posterior
distribution $P(\mat z | \mat x; \vec \theta)$ using Metropolis
sampling. We define the posterior energy, to be minimized, as the
negative log of the posterior distribution $P(z_i | \vec x_i)$. Thus,
Bayes rule implies:
\begin{equation}
  U( z_i = l| \mat x)
  =
   \beta \sum_{j \in \cN_i} T(z_i \neq z_j)
    -
    \log C_p(\kappa_l)
    -
    \kappa_l \vec \mu_l^T \vec x_i
  + \mathrm{const},
  \label{eq:sample}
\end{equation}
which is the sum of the prior energy, the conditional energy, and a
parameter-independent quantity. Then, given a current configuration
$\mat z^n$ Metropolis sampling generates a new candidate label map $\mat w$ as
follows: (i) draw a new label $l'$ at site $i$ with uniform
distribution; $\mat w$ has value $l'$ at site $i$, with other sites
remaining the same as $\mat z^n$; (ii) compute the change of energy
$\Delta U(\mat w) = U(\mat w | \mat x) - U( \mat z^n | \mat x) = U(z_i
= l'| \mat x) - U(z_i = l | \mat x)$; (iii) accept candidate $\mat w$
as $\mat z^{n+1}$ with probability $\min (1, \exp \{ -\Delta U(\mat w)
\})$; (iv) after a sufficiently long burn-in period, generate a sample of size $M$
from the posterior distribution $P(\mat z| \mat x)$.


\subsection{Parameter Estimation}


\textbf {Estimating $\vec \theta_L$:} By maximizing $\widetilde Q_L$
with the constraint $\norm{\vec \mu_l} = 1$ we get
\begin{equation}
  R_l = \sum_{m=1}^{M}\sum_{i \in \cS_l}^{} \vec x_i, \qquad \hat {\vec \mu}_l = \frac{R_l}{\norm{R_l}},
  \label{eq:mlmu}
\end{equation}
where $S_l = \{ i \in \cS: z_i = l\}$ is the set of data points in cluster
$l$. We have no \emph{a priori} knowledge for $\vec \mu_l$, so a maximum
likelihood estimation in \eqref{eq:mlmu} is the best we can do. For $\kappa_l$
we
maximize the posterior distribution $P(\kappa_l | \mat x, \mat z^1, \dots, \mat
z^M) $.  Since $\widetilde Q_L$ is not dependent on $\kappa$, we
maximize $\widetilde Q_L(\kappa_l) + \log P(\kappa_l; \mu_{\kappa},
\sigma_{\kappa}^2)$ and get
\begin{equation}
  A_p(\hat \kappa_l) + \frac{\hat \kappa_l - \mu_{\kappa}}{N_l\sigma_{\kappa}^2} = R_l,
  \label{eq:estkappa}
\end{equation}
where $A_p(\hat \kappa_l) = I_{\frac{p}{2}} (\hat \kappa_l) /
I_{\frac{p}{2}-1} (\hat \kappa_l)$ and $N_l = |\cS_l|$ is the number
of data points in cluster $l$. Because \eqref{eq:estkappa} contains
the ratio of two modified Bessel functions, an analytic solution is
unavailable and we have to resort to a numerical solution. We use
Newton's method for solving $g(\hat \kappa_l) = A_p(\hat \kappa_l)
-(\hat \kappa_l - \mu_{\kappa}) / (N_l\sigma_{\kappa}^2) - R_l= 0$;
the derivative of $A_p(\hat \kappa_l)$ is
\begin{align*}
  A_p(\hat \kappa_l)' = 1 - A_p(\hat \kappa_l)^2 - \frac{p-1}{\hat \kappa_l} A_p(\hat \kappa_l).
\end{align*}
The choice of initial value for Newton's algorithm depends on the
strength of the prior on $\kappa_l$ (i.e. the $\sigma_{\kappa}$
value). For a noninformative prior, $\hat \kappa_l = (pR_l - R^3) / (1
- R^2)$ is a good a good initial value
\cite{banerjee2006clustering}. For a strong prior, a reasonable
initial value is the current value of $\kappa_l$.

\textbf{Estimating $\beta$:} To estimate $\beta$, we again rely on Newton's method
to find the solution numerically. The derivatives $\partial \widetilde Q_P
/ \partial \beta$ and $\partial^2 \widetilde Q_P / \partial \beta^2$ for the
pseudo-likelihood approximation of the MRF prior are easily computed.

\begin{algorithm}[hbt]
  % \DontPrintSemicolon
  \SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
  \Input{Preprocessed 4D fMRI data; number of clusters}
  \Output{labeled functional region map}

  Initialization: Run K-means clustering a few times and choose $\mat z$ with the smallest sum-of-square errors; estimate $\vec \theta_L$ and set
  $\beta$ to a small value\;

  \While{EM not converged}{
    \textbf{E step: } Given current $\vec \theta$,
    \For{$m \leftarrow 1$ \KwTo $M$}{
      \lForEach{$i \in \cS$} {
        Draw sample $z_i^m$ from $P( z_i|\mat x)$ using \eqref{eq:sample}\;
      }
    }
    \textbf{M step: } Given $(\mat z^1,\dots, \mat z^M)$, estimate $\beta$ and $\vec \theta_L$\;
    Estimate labels with ICM, using the current estimates for $\beta$ and $\vec \theta_L$ \;
  }

  \caption{MCEM-ICM Algorithm for Hidden-MRF Model Estimation}
  \label{alg:mcem}
\end{algorithm}

\subsection{MCEM-Based Algorithm for Hidden-MRF Model Estimation}

Given the methods for sampling and parameter estimation, we estimated
the hidden-MRF model by iteratively using (i) MCEM to learn model
parameters and (ii) using ICM to compute optimal region labels. In the
expectation (E) step, we draw samples from the posterior $P(\mat z |
\mat x)$, given current estimates for parameters $\vec \theta$. In the
maximization (M) step, we use these samples to update estimates for
the parameters $\vec \theta$. Algorithm \ref{alg:mcem} describes this
procedure.

\begin{figure}[htb]
  \centering
  \includegraphics[width=0.19\textwidth]{figures/synthetic/true}
  \includegraphics[width=0.19\textwidth]{figures/synthetic/obs}
  \includegraphics[width=0.19\textwidth]{figures/synthetic/sphere1}
  \includegraphics[width=0.19\textwidth]{figures/synthetic/label_icm}
  \includegraphics[width=0.19\textwidth]{figures/synthetic/label_mc}
  \caption{Synthetic example. From left to right is true labels, first
    time point of observed time series, time series plot on sphere,
    label map estimated by ICM, label map estimated by MCEM.}
  \label{fig:toy}
\end{figure}


\section{Results and Conclusion}
\label{sec:exp}


\textbf{Synthetic example:} We first simulate low-dimensional fMRI
data (2D 64$\times$64 image domain; 3 timepoints) to compare the
(i)~proposed method, that combines MCEM-based parameter estimation and
ICM~\cite{zhang2002segmentation}, with (ii)~the mode-approximation approach
that couples maximum-a-posteriori parameter estimation with ICM. We
use a ring-shaped mask to resemble a gray-matter 2D mask. Within this
gray-matter mask, we simulate a label map by sampling from a MRF
having $\beta = 2$. Given the label map, we simulate vMF samples (on a
sphere in 3 dimensions). Figure~\ref{fig:toy} shows that the MCEM solution is
close to the ground truth, while the mode-approximation solution is stuck in a
local maximum.

\noindent{\bf Resting-State fMRI:} We evaluated the proposed method on real
data, from healthy control subjects, in a resting-state fMRI study. BOLD EPI
images (TR = 2.0 s, TE = 28 ms, GRAPPA acceleration factor = 2, 40 slices at 3
mm slice thickness, 64 x 64 matrix, 240 volumes) were acquired on a Siemens 3
Tesla Trio scanner with 12-channel head coil during the resting state (subjects'
eyes were open). The data was preprocessed in SPM~\cite{spmwebsite}, including
motion correction, registration to T2 and T1 structural MR images, spatial
smoothing by a Gaussian filter with $8\times 8 \times 8$ kernel, and masked to
include only the gray matter voxels. We used the \texttt{conn} software
tool~\cite{connwebsite} to regress out signals from the ventricles and white
matter, which have a high degree of physiological artifacts. A bandpass filter
was used to remove frequency components below 0.01 Hz and above 0.1 Hz. We then
projected the data onto the unit sphere by subtracting the mean of each time
series and dividing by the magnitude of the resulting time series. We then
applied the proposed method to estimate the functional network labels. We set
$\mu_{\kappa} = 400$, $\sigma_{\kappa} = 0.5$ (note that this allows the
estimate for cluster concentrations $\kappa_l$ to vary typically from 200-600),
and the number of clusters to 8.

<<<<<<< .mine
Figure~\ref{fig:wholebrain} shows the optimal label maps, produced by
the proposed method on three subjects. We note that among the 8
clusters, one cluster of the largest $\kappa$ value, always has the
most number of voxels that are not in the regions of interest. This
cluster corresponds to background regions with least connectivity and
is not shown in the figure. Among the clusters shown, we can identify
the visual, motor, attention, and default mode
network~\cite{raichle2001}, all in a single label map. We found that
changing the number of clusters, although leads to different label
maps, preserves most of the ROIs corresponding to the
prominent functional connections. For instance, setting the number of
cluster to 4 or 6, also produces optimal label maps containing the
aforementioned four widely-known functional networks (not shown in
figure).
=======
Figure~\ref{fig:wholebrain} shows the optimal label maps, produced by the
proposed method on three subjects. We note that among the 8 clusters, one
cluster, with the largest $\kappa$ value and largest number of voxels,
corresponds to background regions with weakest connectivity and is not shown in
the figure. Among the clusters shown, we can identify the visual, motor, dorsal
attention, executive control, salience, and default mode
networks~\cite{raichle2001}. Four networks, the visual, motor, executive
control, and default mode, were very robustly found across all subjects. More
variability was found in the dorsal attention (notice it is much larger in
subject 3) and salience (notice that it is missing in subject 2). We found that
changing the number of clusters, although leading to different label maps,
preserves the four robust networks. For instance, we also ran the analysis with
the number of clusters set to 4 or 6 (not shown) and were able to recover the
same four robust networks.
>>>>>>> .r162

\begin{figure}[htb]
 \begin{center}
 \begin{tabular}{cccccc}
      \includegraphics[width=0.15\textwidth]{figures/wholebrain/sub1/axial0028} &
      \includegraphics[width=0.15\textwidth]{figures/wholebrain/sub1/axial0034} &
      \vspace{0.5pt}
      \includegraphics[width=0.15\textwidth]{figures/wholebrain/sub2/axial0028} &
      \includegraphics[width=0.15\textwidth]{figures/wholebrain/sub2/axial0034} &
      \vspace{0.5pt}
      \includegraphics[width=0.15\textwidth]{figures/wholebrain/sub5/axial0028} &
      \includegraphics[width=0.15\textwidth]{figures/wholebrain/sub5/axial0034} \\

      \includegraphics[width=0.15\textwidth]{figures/wholebrain/sub1/saggital0029} &
      \includegraphics[width=0.15\textwidth]{figures/wholebrain/sub1/coronal0029} &
      \vspace{0.5pt}

      \includegraphics[width=0.15\textwidth]{figures/wholebrain/sub2/saggital0029} &
      \includegraphics[width=0.15\textwidth]{figures/wholebrain/sub2/coronal0029} &
      \vspace{0.5pt}
      \includegraphics[width=0.15\textwidth]{figures/wholebrain/sub5/saggital0029} &
      \includegraphics[width=0.15\textwidth]{figures/wholebrain/sub5/coronal0029}\\

      \multicolumn{2}{c}{Subject 1} &
      \multicolumn{2}{c}{Subject 2} &
      \multicolumn{2}{c}{Subject 3}
    \end{tabular}
  \end{center}
  \caption {Functional networks detected by the proposed method for 3 subjects
    overlaid on their T1 images. The top row shows two axial slices at different
    positions and the bottom row shows a saggital and coronal view. The clusters
    correpond to the visual (cyan), motor (green), executive control (blue),
    salience (magenta), dorsal attention (yellow), and default mode (red)
    networks.}
  \label{fig:wholebrain}
\end{figure}

<<<<<<< .mine
The next experiment evaluates the consistency of the label map across
multiple subjects, and compares our results with ICA. We apply ICA
toolbox (GIFT v1.3c) \cite{giftwebiste} to the same preprocessed data
that used by our MCEM algorithm. For comparison, we only show one of
the clusters at the medial prefrontal cortex (MPFC) and the posterior
cingulate cortex (PCC), known as the default mode network
(DMN)~\cite{raichle2001}. We choose 8 clusters because it can identify
the major functional networks. The results in figure
\ref{fig:multisub} show the consistency of label map in all 3
subjects. ICA tends to split the DMN into two
components, one with PCC and another with MPFC. It is unclear whether
this is because ICA can not detect DMN for single subjects or because
there is less connectivity between PCC and MPFC. In the proposed
method, the DMN is identified consistently in all three subjects.
=======
The next experiment compares our results with ICA. We applied a standard ICA
toolbox (GIFT v1.3c)~\cite{giftwebiste} to the same preprocessed data used by
our MCEM algorithm. For comparison, we show the clusters for the default mode
network, including the medial prefrontal cortex (MPFC) and the posterior
cingulate cortex (PCC). The results in Figure~\ref{fig:multisub} show the
consistency of MCEM label maps in all 3 subjects. Interestingly, ICA tends to
split the DMN into two components, one with the PCC and another with the
MPFC. The ICA result shows the disadvantage of ignoring spatial information. It
includes several small, likely false-positive regions of connectivity, while the
proposed MCEM method demonstrates better spatial coherence.
>>>>>>> .r162

\begin{figure}[hbt]
  \begin{center}
    \begin{tabular}{llcccc}\footnotesize
      \includegraphics[width=0.14\textwidth]{figures/test1/sub1_pcc1a} &
      \includegraphics[width=0.14\textwidth]{figures/test1/sub1_pcc1s} &
      \vspace{2pt}
      \includegraphics[width=0.14\textwidth]{figures/ica2/c9_sub1a} &
      \includegraphics[width=0.14\textwidth]{figures/ica2/c9_sub1s} &
      \vspace{0.5pt}
      \includegraphics[width=0.14\textwidth]{figures/ica2/c13_sub1a} &
      \includegraphics[width=0.14\textwidth]{figures/ica2/c13_sub1s} \\


      \includegraphics[width=0.14\textwidth]{figures/test1/sub2_pcc1a} &
      \includegraphics[width=0.14\textwidth]{figures/test1/sub2_pcc1s} &
      \vspace{0.8pt}
      \includegraphics[width=0.14\textwidth]{figures/ica2/c9_sub2a} &
      \includegraphics[width=0.14\textwidth]{figures/ica2/c9_sub2s} &
      \vspace{0.5pt}
      \includegraphics[width=0.14\textwidth]{figures/ica2/c13_sub2a} &
      \includegraphics[width=0.14\textwidth]{figures/ica2/c13_sub2s} \\

      \includegraphics[width=0.14\textwidth]{figures/test1/sub5_pcc1a} &
      \includegraphics[width=0.14\textwidth]{figures/test1/sub5_pcc1s} &
      \vspace{0.8pt}
      \includegraphics[width=0.14\textwidth]{figures/ica2/c9_sub5a} &
      \includegraphics[width=0.14\textwidth]{figures/ica2/c9_sub5s} &
      \vspace{0.5pt}
      \includegraphics[width=0.14\textwidth]{figures/ica2/c13_sub5a} &
      \includegraphics[width=0.14\textwidth]{figures/ica2/c13_sub5s}\\

      \multicolumn{2}{c}{ MCEM Method} &
      \multicolumn{2}{c}{ICA comp. 9} &
      \multicolumn{2}{c}{ICA comp. 13}
    \end{tabular}
  \end{center}
  \caption{Comparison of the label map for the default mode network computed by
    the proposed method compared with ICA results for three subjects. Each row
    is one subject. Left: label map estimated by the proposed method. Middle:
    component 9 of ICA. Right: component 13 of ICA. ICA components best matching
    the DMN were chosen.}
  \label{fig:multisub}
\end{figure}


\noindent\textbf{Conclusion: } We present a novel Bayesian approach to labeling
functional networks of the brain from resting-state fMRI that incorporates an
MRF for spatial regularization, and we use MCEM to approximate the maximum
posterior solution. Future work will include extending the model to group
analysis, which can be achieved using a hierarchical Bayesian model. We also
plan to investigae the use of non-parametric Bayesian methods to estimate the
number of clusters.%, as well as dimensionality reduction on the hypersphere.
\label{sec:conc}

\bibliographystyle{splncs03}
\bibliography{reference}
\end{document}
