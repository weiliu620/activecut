\documentclass{article}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsfonts}

\renewcommand{\vec}[1]{\boldsymbol{#1}}
\newcommand{\mat}[1]{\text{\bf #1}}

\author{Bo Wang, Wei Liu, etc}

\title{Active cut based 4D pathological anatomy modeling:
 A TBI imaging study}

\begin{document}
\maketitle
\begin{abstract}
\end{abstract}

\section{Introduction}
\section{Methods}
\subsection{Graph cuts}
% need to make notes about our method's different with reuglar graph
% cuts (or GrabCut) method. We changed the difiniotn of the data term
% of the graph cut's objective function. Instead using the regular
% Gaussian mixture model's marginal defintion, we use the expectation
% of the joint log-likelihood with respet to the hidden variables. The
% benefit of this swith is, now the GMM and graph cuts optimize a
% single unified objective function. Previous GrabCut does not have
% this property. (although it does optimize the single marginal
% likelihood. Here I need to double check the original GrabCut paper).

% Also need to change the defintion of the posterior predictive
% proabbility as a query score. If we just use the smoothness term of
% the graph cuts to define the MRF prior of the predictive
% probability, the model of the predictive probability will be same as
% the model of graph cuts, which is a good thing. However, now the
% prior also includes the data term, and this will violate the
% Bayesian rule.

% we probably need to talk about different parameter settings and its
% impact on the results.

% we may try to do the segmentation even without the user's initial
% bounding box. Is it possible to look at the FLAIR and other channels
% to find the initial large lesion region? Guess we will end up
% finding many false-positive voxels at CSF. How to deal with it?


\noindent \textbf{Graph Cuts global optimization: }We define the objective
function of graph cuts optimization as
% possible to move two energy term in to text. Just keep E(alpha) here.
\begin{align*}
  &\mat E(\alpha) = \gamma \sum_{(m,n)} \psi(\alpha_n, \alpha_m) \exp \left ( -\beta_0 \| \vec x_n - \vec x_m\|^2 \right ) +\sum_n^N \mathbb{E}_{p(\mat z|\mat x)} \log p(\vec x_n, \vec z_n; \theta (\alpha)),\\
  &\mathbb{E}_{p(\mat z|\mat x)} \log p(\vec x, \vec z; \theta (\alpha_n)) = \!\!\sum_{k=1}^K \!z_{nk}(\log \pi_{nk} \!+\! \log \mathcal{N}(\vec x_n; \theta(\alpha_n))) \!+\!  \beta \!\!\!\!\!\!\sum_{m \in \mathcal{N}(n)} \!\!\!\!\!\langle \vec z_n, \!\vec z_m\rangle \!-\! \log C \!\!\!\!,
\end{align*}
where $\psi = 1$ if $\alpha_n \neq \alpha_m$, otherwise $\psi = 0$, and
$\beta_0$ is estimated from data by taking expectation over image
sample~\cite{boykov2001fast,rother2004grabcut}. The smoothness constraint
depends on the data term, so it is a conditional random field. When building the
graph, we set the T-link by the second term, and the N-link by the first term in
the above equation.



\section{Experiments}

\bibliographystyle{plain}
\bibliography{refs}
\end{document}
