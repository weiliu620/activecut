\relax 
\citation{lauritzen1996graphical}
\citation{bishop2006pattern}
\@writefile{toc}{\contentsline {groupheader}{CHAPTERS}{1}}
\@writefile{toc}{\contentsline {chapter}{\numberline {1.}\hyphenpenalty =10000\exhyphenpenalty =10000\relax \linepenalty =0\uppercase {Mathematical Tools}}{1}}
\@writefile{toc}{\contentsline {groupheader}{\vspace {-22pt}}{1}}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chap:math}{{1}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Graphical Model}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Markov Random Field}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces A graph model that represents the Markov chain.\relax }}{2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:mchain}{{1.1}{2}}
\citation{rue2005gaussian}
\citation{hammersley1968markov}
\citation{hammersley1968markov}
\citation{peierls1936ising}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Left: A graphical model representing an MRF. The blue color node is conditional independent of the white node given its adjacent neighbors, colored gray. Right: An MRF defined on a general graph instead of on a regular lattice. The two blue color nodes are conditional independent given the remaining nodes. \relax }}{4}}
\newlabel{fig:mrf}{{1.2}{4}}
\newlabel{eq:ising}{{1.6}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces A simulation of MRF. When a new candidate $w$ is accepted to replace the current $x_s$, we get a new set of variables $X^{m+1}$ that differs from the current variable $X$ at only $s$. The set of variable $X^m$ and $W^{m+1}$ is a sample of a Markov chain, since $X^{m+1}$ depend only on the previous $X^m$. Upon convergence, $X$ will be a sample from the target distribution $P(X)$. \relax }}{5}}
\newlabel{fig:imagechain}{{1.3}{5}}
\citation{metropolis1953equation}
\citation{geman1984stochastic}
\citation{metropolis1953equation}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Simulation of MRF}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.1}Metropolis and Gibbs Sampling}{6}}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Metropolis sampling algorithm for MRF.\relax }}{6}}
\newlabel{alg:metro}{{1}{6}}
\citation{kindermann1980markov}
\newlabel{fig:beta0.8}{{1.4a}{8}}
\newlabel{sub@fig:beta0.8}{{a}{8}}
\newlabel{fig:beta0.88}{{1.4b}{8}}
\newlabel{sub@fig:beta0.88}{{b}{8}}
\newlabel{fig:beta1.0}{{1.4c}{8}}
\newlabel{sub@fig:beta1.0}{{c}{8}}
\newlabel{fig:beta1.5}{{1.4d}{8}}
\newlabel{sub@fig:beta1.5}{{d}{8}}
\newlabel{fig:beta2.0}{{1.4e}{8}}
\newlabel{sub@fig:beta2.0}{{e}{8}}
\newlabel{fig:beta0.88details}{{1.4f}{8}}
\newlabel{sub@fig:beta0.88details}{{f}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces Simulating Ising model with various values of $\beta $. For each simulation, the image is initialized with random states and then scanned 1000 times. Notice when $\beta $ is small, the image is less spatially coherent. When $\beta $ is large, the image has more spatially coherent regions. \relax }}{8}}
\newlabel{fig:isingsim}{{1.4}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.5}{\ignorespaces Simulating Potts model of four states with various values of $\beta $. For all simulations, the image was initialized with random states, and then was scanned 1000 times. \relax }}{8}}
\newlabel{fig:pottssim}{{1.5}{8}}
\citation{barbu2005generalizing}
\citation{wang1987nonuniversal}
\citation{robert2004monte}
\@writefile{loa}{\contentsline {algocf}{\numberline {2}{\ignorespaces Gibbs sampling for MRF.\relax }}{9}}
\newlabel{alg:gibbs}{{2}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.2}Swendsen-Wang Sampling}{9}}
\citation{robert2004monte}
\newlabel{eq:joint}{{1.7}{10}}
\citation{rubinstein2008simulation}
\citation{winkler2003image}
\citation{grimmett2006random}
\citation{barbu2005generalizing}
\citation{huber2003bounding}
\citation{cooper1999mixing}
\citation{mitchell1980need}
\@writefile{lof}{\contentsline {figure}{\numberline {1.6}{\ignorespaces Consecutive samples of Potts model with $\beta = 1.1$ using SW and Gibbs sampling. Top row: Gibbs samples. Bottom: SW samples. Both samplers initialize the sample image with an all-zero image, have 100 burn-in sampling, and then save three consecutive samples. Note that for the SW samples, multiple voxel labels have been changed between the sample images. Such multiple updates speed up convergence. For the Gibbs samples, the three sample images are similar due to the strong interactions (relatively large $\beta $) between the neighboring nodes. \relax }}{12}}
\newlabel{fig:mathsw}{{1.6}{12}}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Hidden Markov Model}{13}}
\newlabel{sec:crf}{{1.4}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.7}{\ignorespaces A graphical representation of a [|<or "the>]hidden Markov model(HMM). $X$ is defined on a regular lattice graph and is given an MRF prior to represent our knowledge of the smoothness or piecewise constant. $Y$ is the observed data that is generated from the likelihood function given the hidden $X$.\relax }}{13}}
\newlabel{fig:hmm}{{1.7}{13}}
\citation{lafferty2001conditional}
\citation{boykov2001interactive}
\citation{rother2004grabcut}
\citation{bishop2006pattern}
\citation{murphy2012machine}
\newlabel{eq:bayes}{{1.8}{14}}
\@writefile{toc}{\contentsline {section}{\numberline {1.5}Inference of Graphical Model and MRF}{14}}
\newlabel{sec:inference}{{1.5}{14}}
\citation{besag1986statistical}
\citation{boykov2001interactive}
\citation{boykov2001fast}
\citation{zhang2002segmentation}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5.1}Iterated Conditional Modes}{15}}
\citation{barbu2005generalizing}
\@writefile{loa}{\contentsline {algocf}{\numberline {3}{\ignorespaces Iterated conditional modes (ICM) for finding the approximate posterior of a discrete random vector given data\relax }}{16}}
\newlabel{alg:icm}{{3}{16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5.2}Sampling}{16}}
\citation{barbu2005generalizing}
\citation{barbu2005generalizing}
\@writefile{loa}{\contentsline {algocf}{\numberline {4}{\ignorespaces Gibbs sampling for the posterior distribution.\relax }}{17}}
\newlabel{alg:gibbspost}{{4}{17}}
\citation{kirkpatrick1983optimization}
\citation{geman1984stochastic}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5.3}Variational Inference}{18}}
\newlabel{sec:variational}{{1.5.3}{18}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.8}{\ignorespaces Simulated annealing samples one variable at a time. Unlike coordinate descent that always moves in the gradient descent direction (blue color arrow), the SA algorithm updates the variable based on a certain probability, which depends on the difference of the function value of two configurations (red arrow).\relax }}{18}}
\newlabel{fig:annealing}{{1.8}{18}}
\citation{bishop2006pattern}
\citation{bishop2006pattern}
\citation{murphy2012machine}
\newlabel{eq:varassume}{{1.5.3}{19}}
\newlabel{eq:varproperty}{{1.14}{19}}
\citation{zhang1992mean}
\newlabel{eq:isingdot}{{1.15}{20}}
\newlabel{eq:mf1}{{1.17}{20}}
\newlabel{eq:mf2}{{1.18}{20}}
\newlabel{eq:mf3}{{1.19}{20}}
\citation{greig1989exact}
\citation{boykov2001fast}
\citation{greig1989exact}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5.4}Graph Cut Optimization}{21}}
\newlabel{eq:gcobj}{{1.20}{21}}
\citation{ford2010flows}
\citation{boykov2001fast}
\@writefile{lof}{\contentsline {figure}{\numberline {1.9}{\ignorespaces Graph cut segmentation. Each voxel is defined as a node on a graph. neighboring voxels have edges between them with weights given by MRF. A source node $s$ and sink node $t$ are added. All nodes has links to both source and sink nodes with weights depends on the likelihood function (data term). Graph cut algorithms find a cut, i.e. a set of edges whose overall weights are minimized. In the figure, edges with solid lines are kept, and edges with dashed lines are removed after the cut. Red think links are the cut. Node is assigned to source or sink label if they are connected to either of them. \relax }}{22}}
\newlabel{fig:graphcuts}{{1.9}{22}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.10}{\ignorespaces Recovering noise image by graph cut. Top row from left to right: observed noised image, ground truth label map, and recovered label map. Bottom: histogram of the observed image intensity. Note the region in blue circle of the true map is mis-classified. \relax }}{23}}
\newlabel{fig:gcexample}{{1.10}{23}}
\@writefile{toc}{\contentsline {section}{\numberline {1.6}Parameter estimation}{24}}
\newlabel{sec:parest}{{1.6}{24}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.11}{\ignorespaces A hidden Markov model with $X$ in MRF, and each $y_s$ is independent Gaussian given $x_s$. The parameters are black dots, the hidden variables are circles, and the observed data are grayed circles. The MRF structure on $X$ is not shown in this diagram. Instead a box is on $X$ and $Y$ to represent the there are $N$ such nodes. \relax }}{24}}
\newlabel{fig:paraest}{{1.11}{24}}
\citation{besag1975statistical}
\citation{dempster1977maximum}
\citation{bishop2006pattern}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6.1}Expectation maximization}{26}}
\newlabel{chap:em}{{1.6.1}{26}}
\newlabel{eq:qfunc}{{1.21}{26}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.12}{\ignorespaces coding scheme for parameter estimation. For 4-neighbors system of 2-D image, the voxels are separated into 4 groups. The voxels in the same group are conditionally independent given other groups.\relax }}{26}}
\newlabel{fig:coding}{{1.12}{26}}
\citation{wei1990monte}
\citation{feodor2000stochastic}
\citation{levine2001implementations}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6.2}Monte Carlo EM}{27}}
\newlabel{c2sec:mcem}{{1.6.2}{27}}
\newlabel{eq:mcqfunc}{{1.22}{27}}
\citation{caffo2002ascent}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6.3}Convergence of MCEM}{28}}
\citation{gelman1992inference}
\newlabel{eq:var}{{1.24}{29}}
\citation{robert2004monte}
\citation{johnson1996studying}
\citation{johnson1996studying}
\citation{gibbs2000bounding}
\citation{johnson1996studying}
\citation{gibbs2000bounding}
\citation{barbu2005generalizing}
\citation{huber2003bounding}
\citation{cooper1999mixing}
\citation{johnson1996studying}
\citation{johnson1996studying}
\@writefile{lof}{\contentsline {figure}{\numberline {1.13}{\ignorespaces Percentile of coupling iterations for Ising model of size $64\times 64$. Top curve shows the 99\% and bottom shows the 95\% percentile from the distribution of the iterations needed for coupling, as a function of $\beta $ parameter. The percentiles are estimated using 1000 repetitions of Gibbs sampling initialized with all white and all-black value. Figure from Johnson\nobreakspace  {}\cite  {johnson1996studying}. \relax }}{32}}
\newlabel{fig:ising}{{1.13}{32}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6.4}Variational inference with EM a.k.a mean field theory}{32}}
\newlabel{chap:mathvar}{{1.6.4}{32}}
\citation{zhang1992mean}
\newlabel{eq:pseudoll}{{1.25}{33}}
\newlabel{eq:mfapp}{{1.26}{33}}
\@setckpt{math_chris}{
\setcounter{page}{34}
\setcounter{equation}{26}
\setcounter{enumi}{0}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{1}
\setcounter{figure}{13}
\setcounter{table}{0}
\setcounter{oldchapter}{0}
\setcounter{oldtocdepth}{0}
\setcounter{thrm}{0}
\setcounter{parentequation}{0}
\setcounter{mytheorem}{0}
\setcounter{myproposition}{0}
\setcounter{mydefinition}{0}
\setcounter{mycorollary}{0}
\setcounter{myexample}{0}
\setcounter{myexercise}{0}
\setcounter{AlgoLine}{0}
\setcounter{algocfline}{4}
\setcounter{algocfproc}{4}
\setcounter{algocf}{4}
\setcounter{ALC@unique}{0}
\setcounter{ALC@line}{0}
\setcounter{ALC@rem}{0}
\setcounter{ALC@depth}{0}
\setcounter{ContinuedFloat}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{mydef}{3}
\setcounter{section}{6}
\setcounter{subsection}{4}
\setcounter{paragraph}{0}
\setcounter{subsubsection}{0}
}
