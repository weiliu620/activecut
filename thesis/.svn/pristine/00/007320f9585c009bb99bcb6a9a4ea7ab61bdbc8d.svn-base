\chapter{Identify Consistent, Spatially Coherent Multiple Functional Networks}
\label{chap:method2}
We propose a novel Bayesian framework for partitioning the cortex into distinct
functional networks based on resting-state fMRI. Unlike the previous chapter
where we aim to esimate the pairwise connectivity between voxels by using MRF,
this chapter comes back to the original image space and aim to cluster the
voxels with high connectivity into same cluster. 

We again model the spatial coherence within the network clusters using a Markov
random field prior. The normalized time-series data, which lie on a
high-dimensional sphere, are modeled with a mixture of von Mises-Fisher
distributions. To estimate the parameters of this model, we maximize the
posterior using a Monte Carlo expectation maximization (MCEM) algorithm in which
the intractable expectation over all possible labelings is approximated using
Monte Carlo integration. We show that MCEM solutions on synthetic data are
superior to those computed using a mode approximation of the expectation
step. Finally, we demonstrate on real fMRI data that our method is able to
identify visual, motor, salience, and default mode networks with considerable
consistency between subjects.

%% Resting-state functional magnetic resonance imaging (fMRI) measures background
%% fluctuations in the blood oxygen level-dependent (BOLD) signal of the brain at
%% rest. The temporal correlations between these signals are used to estimate the
%% functional connectivity of different brain regions. This technique has shown
%% promise as a clinical research tool to describe functional abnormalities in
%% Alzheimer's disease, schizophrenia, and autism \cite{fox2010clinical}. 
In resting-state fMRI, a standard analysis procedure is to select a region of
interest (ROI), or seed region, and find the correlation between the average
signal of the ROI and other voxels in the brain. These correlations are
thresholded so that only those voxels with significant correlations with the
seed region are shown. Recent methods to find functional networks without seed
regions include independent component analysis
(ICA)~\cite{beckmann2005tensorial}, which often includes an ad hoc method to
manually choose those components that are anatomically meaningful. Other
approaches employ clustering techniques to automatically partition the brain
into functional networks. A similarity metric is defined, e.g., correlation
\cite{5074650} or frequency coherence \cite{thirion2006detection}, and then a
clustering method such as $k$-means or spectral clustering is used to group
voxels with similar time series. A drawback of these approaches is that they
disregard the spatial position of voxels, and thus ignore the fact that
functional networks are organized into sets of spatially coherent regions.

We introduce a new data-driven method to partition the brain into networks of
functionally-related regions from resting-state fMRI. The proposed algorithm
does not require specification of a seed, and there is no ad hoc thresholding or
parameter selection. We make a natural assumption that functionally homogeneous
regions should be spatially coherent. Our method incorporates spatial
information through a Markov random field (MRF) prior on voxel labels, which
models the tendency of spatially-nearby voxels to be within the same functional
network. Each time series is first normalized to zero mean and unit norm, which
results in data lying on a high-dimensional unit sphere. We then model the
normalized time-series data as a mixture of von Mises-Fisher (vMF) distributions
\cite{banerjee2006clustering}. Each component of the mixture model corresponds
to the distribution of time series from one functional network. Solving for the
parameters in this combinatorial model is intractable, and we therefore use a
stochastic method called Monte Carlo Expectation Maximization (MCEM), which
approximates the expectation step using Monte Carlo integration. The stochastic
property of MCEM makes it possible to explore a large solution space, and it
performs better than a standard mode approximation method using iterated
conditional modes (ICM).

The proposed method is related to previous approaches using MRFs to model
spatial relationships in fMRI data. Descombes et
al.~\cite{descombes_spatio-temporal_1998} use a spatio-temporal MRF to analyze
task-activation fMRI data. Liu et al.~\cite{liu2010spatial} use an MRF model of
resting state fMRI to estimate pairwise voxel connections. However, neither of
these approaches tackle the problem of clustering resting-state fMRI into
functional networks. The previous chapter of this thesis estimate the functional
connectivities between each pair of voxels. Once the estimation is done, we need
to choose a seed region in order to show the connected regions with this
seed. Instead, the method we proposed here estimate the clusters which constain
all voxels taht belong to the same functional network. Users do not need to give
a seed regions in order to identify the netwrok strucutre. This is in spirit
similar to the functional parcellation work that has been done by Yeo
et. al.\cite{yeo2011organization}
\section {Hidden Markov Models of Functional Networks}\label{sec:models}
% Bayesian, prior, likelihood
We use a Bayesian statistical framework to identify functional networks of the
gray matter in fMRI data. We formulate a generative model, which first generates
a spatial configuration of functional networks in the brain, followed by an fMRI
time series for each voxel based on its network membership. We employ an MRF
prior to model network configurations, represented by the hidden network label
variables. Given a label, we assume that the fMRI time series, normalized to
zero mean and unit norm, are drawn from a von Mises-Fisher distribution.

% notation: indices, labels
Let $\cV$ be the set of indices for all gray-matter voxels. We assume that the
number of networks $L$ is a known free parameter. Let $\cL = \{1, 2, \cdots,
L\}$ be the set of labels, one for each network. We denote a label map for
functionally-connected networks as a vector $\vec x = (x_1,\dots, x_N), x_s \in
\cL$.  Let $\cX = \cL^N$ be the set of all possible $\vec x$'s configurations.

\subsection{Markov Prior Model}
Functional networks should consist of few, reasonably-sized, possibly distant
regions. We model such networks $\vec x$ using a special case of MRF model,
i.e. the \emph{Potts}~\cite{LiBook}:
\begin{equation*}
  P(\mat z)  =  \frac {1} {C} \exp \left \{ -\beta \sum_{(r,s)\in \cE}\psi(x_r, x_s) \right \},
\end{equation*}
where the function $\psi$ takes $1$ if its argument is not equal and $0$
otherwise; $(r,s)$ is the set of voxel pairs that are spatial neighbors on the
graph; $\beta > 0$ is a model parameter controlling the strength of label-map
smoothness; $Z$ is a normalization constant that is the sum of $P(\vec x)$ over
all possible configuration of $\vec x$. The Markov-Gibbs
equivalence~\cite{LiBook} implies that the conditional distribution of $x_s$ at
site $s$ is:
\begin{equation}
  \label{eq:mrfprior}
  P(x_s | \vec x_{-s})  = P(x_s | x_{\cN_s}) =  \frac {
    \exp \left\{ -\beta \sum_{r \in \cN_s} \psi(x_s, x_r) \right\}
  }
  {
    \sum_{l \in \cL} \exp \left \{ -\beta \sum_{r \in \cN_s} \psi(x_r, l) \right\}
  },
\end{equation}
where $\vec z_{-s}$ is the collection of all variables in $\vec z$ excluding
site $s$. The neighborhood is the usual 6 adjacent voxels, which does not overly
smooth across boundaries. Previous works
\cite{wei1990monte,descombes_spatio-temporal_1998} have demonstrated the
advantages of MRF's over Gaussian smoothing in preserving segment boundaries.

\subsection {Likelihood Model}
% normalization
We observe that in ordert to make the analysis robust to shifts or
scalings of the data, one typically normalize the time series at each voxel to
zero mean and unit length. This results in the data being projected onto a
high-dimensional unit sphere. After normalization, the sample correlation
between two time series is equal to their inner product, or equivalently, the
cosine of the geodesic distance between these two points on the sphere. Thus, we
re-formulate the problem of finding clusters of voxels with high correlations to
the problem of finding clusters with small within-cluster distances on the
sphere.

\begin{figure}[tbh] 
  \centering 
  \begin{subfigure}[t]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/method2/vmf}
    \caption{Two vectors on 1-D sphere.}
    \label{fig:vmf}
    \end{subfigure}
~
  \begin{subfigure}[t]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/method2/sphere1}
    \caption{time series data on high-D sphere.}
    \label{fig:sphere}
    \end{subfigure}
  \caption{Data points with Von Mises-Fisher distribution.}
  \label{fig:vmfdata}
\end{figure}

% likelihood
In previous chapter, the observed data is the linear correlations between a
paior of voxels BOLD singal. Since the correlation is a scalar, the emission
fucntion, i.e. the conditional probability of $P(\vec y | \vec x)$ can be easily
modeled by the Gaussian distribution once the correlation has been
Fisher-transformed. In the model of this chapter, the observed data is the BOLD
signal itself, so we need a multivariate distribution to model its conditional
probability.  We use the notation $\vec y = \{ (y_1, \dots, y_N)\, |\, y_s \in
S^{p-1} \}$ to denote the set of \emph{normalized} time series. Observe that
given $\vec x \in \cX$, the random vectors $ y_s, \forall s \in \cV$ are
conditional independent. Thus, the likelihood $\log P(\vec y | \vec x) = \sum_{s
  \in \cV} \log P (\vec y_s | x_s)$. We model the emission function $P(y_s
| x_s)$ using the von Mises-Fisher distribution
\begin{equation}
  f ( y_s;\mu_l, \kappa_l | x_s = l)  = C_p(\kappa_l)
  \exp (\kappa_l \mu_l^{\intercal} y_s),
  \quad   y_s \in S^{p-1}, \quad l \in {\cL},
  \label{eq:vmf}
\end{equation}
where, for the cluster labeled $l$, $\mu_l$ is the mean
direction, $\kappa_l \geq 0$ is the \emph{concentration parameter},
and the normalization constant $C_p(\kappa) = {\kappa^{\frac{p}{2} -
    1}}/((2\pi)^{\frac{p}{2}} I_{\frac{p}{2}-1}(\kappa))$, where
$I_\nu$ denotes the modified Bessel function of the first kind with
order $\nu$. The larger the $\kappa$, the greater is the density
concentrated around the mean direction. Since \eqref{eq:vmf} depends
on $y$ only by $\mu^{\intercal} y$, the vMF distribution is
unimodal and rotationally symmetric around $\mu$.

% 'prior' on kappa
In the Bayesian framework, we also define distributions on parameters. We assume
that $\forall l \in \cL$, $\kappa_l \sim \cN(\mu_{\kappa}, \sigma_{\kappa}^2)$
with hyperparameters $\mu_{\kappa}$ and $\sigma_{\kappa}^2$ that can be set
empirically. This prior enforces constraints that the clusters should not have
extremely high or low concentration parameters. We empirically tune the
hyperparameters $\mu_{\kappa}$ and $\sigma_{\kappa}^2$ and have found the
results to be robust to specific choices of the hyperparameters.

\subsection{Monte Carlo EM}
\label{sec:mcem}


\begin{figure}[tbh] 
  \centering
  \includegraphics[width=0.5\textwidth]{figures/method2/genevmf}
  \caption{A generative model of the functional network. The network variables
    $\vec x$ is multivariate variables defined on a MRF. Given $\vec x$, $\vec
    y$ is seen as being generated from a vMF distribution whose parameter $\mu$ and
    $\kappa$ are functions of $\vec x$.}
  \label{fig:genevmf}
\end{figure}
The model of the functional network variables can be illustrated by Figure
\ref{fig:genevmf}. Give the definition of the prior and likelihood function,
our goal is the statistical inference of the posterior probability of $\vec x$
given the observed data $y$. Because of both the hidden variables $\vec x$ and
the model parameters $\mu, \kappa, \beta$ are unknow, we use expectation
maximization (EM) method to estimate both in a iterative way.  To estimate the
model parameters and the hidden labels, we use a stochastic variant of
expectation maximization (EM) called Monte Carlo EM
(MCEM)~\cite{wei1990monte}. The standard EM algorithm maximizes the expectation
of the log-likelihood of joint pdf of $\vec y$ and the hidden variable $\vec x$
with respect to the posterior probability $P(\vec x | \vec y)$,
i.e. $\mathbb{E}_{P(\vec x| \vec y)} [\log P(\vec x, \vec y; \vec \theta)]$. The
combinatorial number of configurations for $\vec x$ makes this expectation
intractable. Thus, we use Monte Carlo simulation to approximate this expectation
as
\begin{equation}
  \widetilde Q(\vec \theta; \vec x, \vec y)
  \approx
  \frac
  {1}
  {M}
  \sum_{m=1}^{M}
    \log
    P (\vec x^m; \beta)
    +
    \log
    P (\vec y | \vec x^m; \vec \theta_L),
  \label{eq:mcemq}
\end{equation}
where $\vec x^m$ is a sample from $P(\vec x | \vec y)$, $\vec \theta_L = \{\vec
\mu_l, \kappa_l : l \in \cL\}$ is the parameter vector of the likelihood, and
$\vec \theta=\{\beta, \vec \theta_L\}$ is the full parameter vector of the
model. Computing the MRF prior in \eqref{eq:mcemq} is still intractable due to
the normalization constant, and we instead use a pseudo-likelihood
approximation~\cite{LiBook}, which gives
\begin{align*}
\centering
  \widetilde Q
  &
  \approx
  \frac{1}{M}
  \sum_{m=1}^{M}
  \sum_{s \in \cV}
  \log P(x_s | x_{\cN_s}; \beta)
  +
  \frac{1}{M}
  \sum_{m=1}^{M}
  \sum_{s \in \cV}
  \log
  P( y_s | x_s; \vec \theta_L)
  =
  \widetilde Q_P
  +
  \widetilde Q_L.
\end{align*}
We use $\widetilde Q_P$ to denote the log-pseudo-likelihood of the prior
distribution, and use $\widetilde Q_L$ to denote the log-likelihood
distribution. Now the prior term $\widetilde Q_P$ is in a tractable form for
evaluation, and we can estimate $\beta$ by optimization of $\widetilde Q_P$ with
a Newton-Raphson mehtod. Because of the separation of the parameters in
$\widetilde Q_P$ and $\widetilde Q_L$, we can estimate the parameter $\vec
\theta_L$ by optimizaing $Q_L$. This is a closed form solution same to 

\subsection{Sampling from the Posterior}
Given the observed data $\vec y$ and parameter value $\vec \theta =
\{\beta, \vec \theta_L \}$, we sample from the posterior
distribution $P(\vec x | \vec y; \vec \theta)$ using Metropolis
sampling. We define the posterior energy, which is to be minimized, as the
negative log of the posterior $P(x_s | \vec y_s)$. Thus,
Bayes rule implies:
\begin{equation}
  U( x_s = l| \vec x)
  =
   \beta \sum_{r \in \cN_s} \psi(x_s, x_r) - \log C_p(\kappa_l)
    -
    \kappa_l \mu_l^{\intercal}  y_s
  + \mathrm{const},
  \label{eq:sample}
\end{equation}
which is the sum of the prior energy, the conditional energy, and a
parameter-independent quantity. Then, given a current configuration $\vec x^m$,
Metropolis sampling generates a new candidate label map $\vec w$ as follows: (i)
draw a new label $l'$ at site $s$ with uniform distribution; $\vec w$ has value
$l'$ at site $s$, with other sites remaining the same as $\vec x^n$; (ii)
compute the change of energy $\Delta U(\vec w) = U(\vec w | \vec y) - U( \vec
x^m | \vec y) = U(x_s = l'| \vec y) - U(x_s = l | \vec y)$; (iii) accept
candidate $\vec w$ as $\vec x^{m+1}$ with probability $\min (1, \exp \{ -\Delta
U(\vec w) \})$; (iv) after a sufficiently long burn-in period, generate a sample
of size $M$ from the posterior distribution $P(\vec x| \vec y)$. This is indeed
the Metropolis sampling algorithm \ref{alg:metropolis} that we have introduced
in chapter \ref{chap:math}.
\subsection{Parameter Estimation}


\subsection{Estimating $\vec \theta_L$}
In order to esimate $\mu$ and $\kappa$ of the vMF distribution, we need to
maximize $\widetilde Q_L$ with the constraint $\norm{\vec \mu_l} = 1$ and
$\kappa > 0$. Because of each data point $y_s$ in the observed dataset $\vec y$
is independent, and also the Monte Carlo samples $\vec y^m$ are also assumed
indepedent, we can estimate $\mu$ 
as~\cite{banerjee2006clustering,dhillon2003modeling}
\begin{equation}
  R_l = \sum_{m=1}^{M}\sum_{s \in \cV_l}^{} \vec y_x, \qquad \hat {\mu}_l = \frac{R_l}{\norm{R_l}},
  \label{eq:mlmu}
\end{equation}
where $\cV_l = \{ s \in \cV: x_s = l\}$ is the set of data points in cluster
$l$. We have no \emph{a priori} knowledge for $\vec \mu_l$, so a maximum
likelihood estimation in \eqref{eq:mlmu} is the best we can do. For $\kappa_l$
we maximize the posterior distribution $P(\kappa_l | \vec y, \vec x^1, \dots,
\vec x^M) $.  Since $\widetilde Q_P$ is not dependent on $\kappa$, we maximize
$\widetilde Q_L(\kappa_l) + \log P(\kappa_l; \mu_{\kappa}, \sigma_{\kappa}^2)$
and get
\begin{equation}
  A_p(\hat \kappa_l) + \frac{\hat \kappa_l - \mu_{\kappa}}{N_l\sigma_{\kappa}^2} = R_l,
  \label{eq:estkappa}
\end{equation}
where $A_p(\hat \kappa_l) = I_{\frac{p}{2}} (\hat \kappa_l) / I_{\frac{p}{2}-1}
(\hat \kappa_l)$ and $N_l = |\cV_l|$ is the number of data points in cluster
$l$. Because \eqref{eq:estkappa} contains the ratio of two modified Bessel
functions, an analytic solution is unavailable and we have to resort to a
numerical solution. We use Newton's method for solving $g(\hat \kappa_l) =
A_p(\hat \kappa_l) -(\hat \kappa_l - \mu_{\kappa}) / (N_l\sigma_{\kappa}^2) -
R_l= 0$. The choice of initial value for Newton's algorithm depends on the
strength of the prior on $\kappa_l$ (i.e. the $\sigma_{\kappa}$ value). For a
noninformative prior, $\hat \kappa_l = (pR_l - R^3) / (1 - R^2)$ is a good a
good initial value \cite{banerjee2006clustering}. For a strong prior, a
reasonable initial value is the current value of $\kappa_l$.

\textbf{Estimating $\beta$:} To estimate $\beta$, we again rely on Newton's
method to find the solution numerically. The derivatives $\partial \widetilde
Q_P / \partial \beta$ and $\partial^2 \widetilde Q_P / \partial \beta^2$ for the
pseudo-likelihood approximation of the MRF prior are easily computed. With all
the settings above, we give the steps of the E step and M step iterations in
algorithm~\ref{alg:mcem}.

\begin{algorithm}[hbt]
  % \DontPrintSemicolon
  \SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
  \Input{Preprocessed 4D fMRI data; number of clusters}
  \Output{Labeled functional network map}

  Initialization: Run $k$-means clustering a few times and choose $\mat z$ with the smallest sum-of-square errors; estimate $\vec \theta_L$ and set
  $\beta$ to a small value\;

  \While{MCEM not converged}{
    \textbf{E step: } Given current $\vec \theta$,
    \For{$m \leftarrow 1$ \KwTo $M$}{
      \lForEach{$s \in \cV$} {
        Draw sample $x_s^m$ from $P( x_s|\vec y_s)$ using \eqref{eq:sample}\;
      }
    }
    \textbf{M step: } Given $(\vec x^1,\dots, \vec z^M)$, estimate $\beta$ and $\vec \theta_L$\;
    Estimate labels with ICM  using the current estimates for $\beta$ and $\vec \theta_L$ \;
  }

  \caption{MCEM-ICM Algorithm for Hidden-MRF Model Estimation}
  \label{alg:mcem}
\end{algorithm}

Given the methods for sampling and parameter estimation, we estimated the
hidden MRF model by iteratively using (i) MCEM to learn model parameters and
(ii) using ICM to compute optimal network labels. In the expectation (E) step,
we draw samples from the posterior $P(\vec x | \vec y)$, given current estimates
for parameters $\vec \theta$. In the maximization (M) step, we use these samples
to update estimates for the parameters $\vec \theta$.

\begin{figure}[htb]
  \centering
  \includegraphics[width=0.19\textwidth]{figures/method2/synthetic/true}
  \includegraphics[width=0.19\textwidth]{figures/method2/synthetic/obs}
  \includegraphics[width=0.19\textwidth]{figures/method2/synthetic/label_icm}
  \includegraphics[width=0.19\textwidth]{figures/method2/synthetic/label_mc}
  \caption{Synthetic example. From left to right: true labels, first
    time point of observed time series, time series plot on sphere,
    label map estimated by mode-approximation, and label map estimated by MCEM.}
  \label{fig:toy}
\end{figure}

\section{Experiments Results}
\label{sec:exp}

In this section, we give some preliminary test result on simulated fMRI dataset,
in order to show its accuracy under large noise. We then apply our MCEM method
on the \emph{in vivo} data.

\subsection{Synthetic example} 
We first simulate low-dimensional time series (2D 64$\times$64 image domain; 3
timepoints, for visualization on sphere $S^2$) to compare the (i)~proposed
method using MCEM with (ii)~the mode-approximation approach that replaces the E
step in EM with a mode approximation. We simulate a label map by sampling from a
MRF having $\beta = 2$ and number of labels $L=4$. Given the label map, we
simulate vMF samples on the sphere $S^2$. Figure~\ref{fig:toy} gives the
simulated data and the estimation by the two methods. We note the simulated
network label map is piecewise constant, with occisional small regions scattered
between large patches. This is similar to the estimated functional network maps
from the real data. Although there are only 3 time points, it is difficult to
visualize the dynamic change of the voxel intensity over time, so we choose to
show only the volume at the first time point on the second image of
Figure~\ref{fig:toy}. Both the ICM and MCEM model is initialized with a K-Means
clustering. We observe that ICM easily converges to a local minimum of the
energy function within a few iterations, and its estimated labels are less
accurate compared with our MCEM approach. The MCEM solution is close to the
ground truth, while the mode-approximation solution is stuck in a local maximum.

\subsection{Resting-State fMRI} 
We evaluated the proposed method on real data obtained from healthy control
subjects, in a resting-state fMRI study. BOLD EPI images (TR = 2.0 s, TE = 28
ms, 40 slices at 3 mm slice thickness, 64 x 64 matrix, 240 volumes) were
acquired on a Siemens 3 Tesla Trio scanner. The data was preprocessed in SPM,
including motion correction, registration to T2 and T1 structural MR images,
spatial smoothing by a Gaussian filter, and masked to include only the
gray-matter voxels. We used the \texttt{conn}
software~\footnote{http://www.nitrc.org/projects/conn/} to regress out signals
from the ventricles and white matter, which have a high degree of physiological
artifacts. A bandpass filter was used to remove frequency components below 0.01
Hz and above 0.1 Hz. We then projected the data onto the unit sphere by
subtracting the mean of each time series and dividing by the magnitude of the
resulting time series. We then applied the proposed method to estimate the
functional network labels with the number of clusters set to $L = 8$.

Figure~\ref{fig:wholebrain} shows the optimal label maps, produced by the
proposed method for 3 of all 16 subjects in the dataset. We note that among the
8 clusters, one cluster, with the largest $\kappa$ value and largest number of
voxels, corresponds to background regions with weakest connectivity and is not
shown in the figure. Among the clusters shown, we can identify the visual,
motor, dorsal attention, executive control, salience, and default mode networks
(DMN)~\cite{raichle2001}. Four networks: the visual, motor, executive control,
and DMN, were robustly found across all subjects. More variability was found in
the dorsal attention network (notice that it is much larger in subject 3) and
salience network (notice that it is missing in subject 2). We found that
changing the number of clusters, although leading to different label maps,
preserves the four robust networks. For instance, we also ran the analysis with
the number of clusters set to 4 or 6 (results not shown) and were able to
recover the same four robust networks.


\begin{figure}[htb]
  \centering
  \includegraphics[width=0.15\textwidth]{figures/method2/wholebrain/sub1/axial0028} 
  \includegraphics[width=0.15\textwidth]{figures/method2/wholebrain/sub1/axial0034} 
  \includegraphics[width=0.15\textwidth]{figures/method2/wholebrain/sub1/saggital0029} 
  \includegraphics[width=0.15\textwidth]{figures/method2/wholebrain/sub1/coronal0029} \\

  \includegraphics[width=0.15\textwidth]{figures/method2/wholebrain/sub2/axial0028} 
  \includegraphics[width=0.15\textwidth]{figures/method2/wholebrain/sub2/axial0034} 
  \includegraphics[width=0.15\textwidth]{figures/method2/wholebrain/sub2/saggital0029} 
  \includegraphics[width=0.15\textwidth]{figures/method2/wholebrain/sub2/coronal0029} \\

  \includegraphics[width=0.15\textwidth]{figures/method2/wholebrain/sub5/axial0028} 
  \includegraphics[width=0.15\textwidth]{figures/method2/wholebrain/sub5/axial0034} 
  \includegraphics[width=0.15\textwidth]{figures/method2/wholebrain/sub5/saggital0029} 
  \includegraphics[width=0.15\textwidth]{figures/method2/wholebrain/sub5/coronal0029}
  \caption {Functional networks detected by the proposed method for 3 subjects
    overlaid on their T1 images.  The clusters are the visual (cyan), motor
    (green), executive control (blue), salience (magenta), dorsal attention
    (yellow), and default mode (red) networks.}
  \label{fig:wholebrain}
\end{figure}

\begin{table}[bh]
  \centering
  \caption{The number of voxels with value greater than 8 in the overlapped label map. }
  \begin{tabular*}{0.75\textwidth}{@{\extracolsep{\fill}} l  r r r r}
    & DMN & Motor & Attention & Visual \\
    \hline
    MCEM    & 5043  & 7003 & 3731 & 5844 \\
    Individual ICA & 114 & 167 & 228 & 134 \\
    Group ICA & 3075 & 5314 & 3901 & 3509
  \end{tabular*}\label{table:agreement}
\end{table}

The next experiment compares our results with ICA. A standard ICA toolbox (GIFT;
\url{mialab.mrn.org}) was applied on the same preprocessed data of each subject
independently, which we call ``Individual ICA''. We also applied standard Group
ICA, using all data from the 16 subjects simultaneously. In both ICA experiments
the number of components are set to 16. The component maps are converted to z
score and thresholded at 1. For each method we computed an overlap map for each
functional network by adding the corresponding binary label maps of all 16 subjects.
The results in Figure~\ref{fig:multisub} show our method can detect the motor,
attention, and visual network with accuracy comparable with Group ICA. Besides,
our method also detects DMN with posterior cingulate cortex (PCC) and medial
prefrontal cortex (MPFC), while Group ICA split the DMN into two components, one
with the MPFC and another with the PCC (not shown).

To see the consistency of the label map between subjects for all three methods,
we look at each method's overlapped label map and count the number of voxels
whose value are greater than 8. Table~\ref{table:agreement} shows that our
method exhibits better consistency than both Individual and Group ICA.

\begin{figure}[tb]
  \centering
  \begin{subfigure}[b]{1\textwidth}
    \centering
      \includegraphics[height=0.14\textwidth]{figures/method2/mcem/dmn_a} 
      \includegraphics[height=0.14\textwidth]{figures/method2/mcem/dmn_s} 
      \includegraphics[height=0.14\textwidth]{figures/method2/ica_separate/DMN_a} 
      \includegraphics[height=0.14\textwidth]{figures/method2/ica_separate/DMN_s}
      \includegraphics[height=0.14\textwidth]{figures/method2/ica_single/dmn_a} 
      \includegraphics[height=0.14\textwidth]{figures/method2/ica_single/dmn_s} 
      \caption{DMN}
      \label{fig:mcemdmn}
  \end{subfigure}

  \begin{subfigure}[b]{\textwidth}
    \centering
    \includegraphics[height=0.14\textwidth]{figures/method2/mcem/motor_a} 
    \includegraphics[height=0.14\textwidth]{figures/method2/mcem/motor_s} 
    \includegraphics[height=0.14\textwidth]{figures/method2/ica_separate/motor_a} 
    \includegraphics[height=0.14\textwidth]{figures/method2/ica_separate/motor_s}
    \includegraphics[height=0.14\textwidth]{figures/method2/ica_single/motor_a} 
    \includegraphics[height=0.14\textwidth]{figures/method2/ica_single/motor_s} 
    \caption{motor}
    \label{fig:mcemmotor}
  \end{subfigure}

  \begin{subfigure}[b]{\textwidth}
    \centering
    \includegraphics[height=0.14\textwidth]{figures/method2/mcem/visual_a} 
    \includegraphics[height=0.14\textwidth]{figures/method2/mcem/visual_s} 
    \includegraphics[height=0.14\textwidth]{figures/method2/ica_separate/visual_a} 
    \includegraphics[height=0.14\textwidth]{figures/method2/ica_separate/visual_s}
    \includegraphics[height=0.14\textwidth]{figures/method2/ica_single/visual_a} 
    \includegraphics[height=0.14\textwidth]{figures/method2/ica_single/visual_s} 
    \caption{visual}
    \label{fig:mcemvisual}
  \end{subfigure}

  \begin{subfigure}[b]{\textwidth}
    \centering
    \includegraphics[height=0.14\textwidth]{figures/method2/mcem/atten_a} 
    \includegraphics[height=0.14\textwidth]{figures/method2/mcem/atten_s} 
    \includegraphics[height=0.14\textwidth]{figures/method2/ica_separate/atten_a} 
    \includegraphics[height=0.14\textwidth]{figures/method2/ica_separate/atten_s}
    \includegraphics[height=0.14\textwidth]{figures/method2/ica_single/atten_a} 
    \includegraphics[height=0.14\textwidth]{figures/method2/ica_single/atten_s} 
    \caption{attentive}
    \label{fig:mcematten}
  \end{subfigure}
  \caption{Comparision of the oerlap of the label maps estimated by our MCEM
    approach, group ICA and single subject ICA on 16 subjets. Left: MCEM
    methods. Middle: single subject ICA. Right: group ICA. Colr map ranges from
    8 (red) 16 (yellow). }
  \label{fig:multisub}
\end{figure}

\section{Discussions}
ICA is inherently a signal decomposition method. It decompose the time series
singal into either spatial or temporal independent components. The estimated
components will be continuous number, thresholded at certain p-value. The MCEM
segmentation, on the other hand, is a image segmentation method. The output of
the MCEM algorithm is a label map, and heach label represent the functional
component of the current voxel. Although it is difficult to have a direct
comparision of the two methods, we can compute the average network structures
across multiple subjects by the two methods. It is shown that because of the
spatial coherence of MCEM, the estimated functional network maps are more
similar across subjects. therefore, the average network maps have less
variation.

