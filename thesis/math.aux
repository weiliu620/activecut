\relax 
\citation{lauritzen1996graphical}
\citation{bishop2006pattern}
\@writefile{toc}{\contentsline {chapter}{\numberline {3.}\hyphenpenalty =10000\exhyphenpenalty =10000\relax \linepenalty =0\uppercase {Mathematical Tools}}{25}}
\@writefile{toc}{\contentsline {groupheader}{\vspace {-22pt}}{25}}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chap:math}{{3}{25}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Graphical model}{25}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Markov random field}{26}}
\citation{rue2005gaussian}
\citation{clifford1990markov}
\citation{kollar2009probabilistic}
\citation{peierls1936ising}
\citation{potts1952some}
\citation{rue2005gaussian}
\newlabel{eq:ising}{{3.6}{28}}
\citation{metropolis1953equation}
\citation{geman1984stochastic}
\citation{metropolis1953equation}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Simulation of MRF}{29}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Metropolis and Gibbs sampling}{29}}
\newlabel{sec:mathsampling}{{3.3.1}{29}}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Metropolis sampling algorithm for MRF.\relax }}{30}}
\newlabel{alg:metro}{{1}{30}}
\citation{kindermann1980markov}
\citation{barbu2005generalizing}
\citation{wang1987nonuniversal}
\citation{robert2004monte}
\citation{robert2004monte}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Swendsen-Wang sampling}{32}}
\newlabel{eq:joint}{{3.7}{32}}
\@writefile{loa}{\contentsline {algocf}{\numberline {2}{\ignorespaces Gibbs sampling for MRF.\relax }}{33}}
\newlabel{alg:gibbs}{{2}{33}}
\citation{rubinstein2008simulation}
\citation{rubinstein2008simulation}
\citation{winkler2003image}
\citation{grimmett2006random}
\citation{barbu2005generalizing}
\citation{huber2003bounding}
\citation{cooper1999mixing}
\citation{mitchell1980need}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Hidden Markov model}{35}}
\newlabel{sec:crf}{{3.4}{35}}
\citation{lafferty2001conditional}
\citation{boykov2001interactive}
\citation{rother2004grabcut}
\newlabel{eq:bayes}{{3.8}{36}}
\citation{bishop2006pattern}
\citation{murphy2012machine}
\citation{besag1986statistical}
\citation{boykov2001interactive}
\citation{boykov2001fast}
\citation{zhang2001segmentation}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Inference of graphical model and MRF}{37}}
\newlabel{sec:inference}{{3.5}{37}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.1}Iterated conditional modes}{37}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.2}Sampling}{38}}
\@writefile{loa}{\contentsline {algocf}{\numberline {3}{\ignorespaces Iterated conditional modes (ICM) for finding approximate posterior of discrete random vector given data\relax }}{39}}
\newlabel{alg:icm}{{3}{39}}
\citation{barbu2005generalizing}
\citation{barbu2005generalizing}
\citation{barbu2005generalizing}
\citation{barbu2005generalizing}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.3}Simulated annealing}{40}}
\@writefile{loa}{\contentsline {algocf}{\numberline {4}{\ignorespaces Gibbs sampling for the posterior distribution.\relax }}{41}}
\newlabel{alg:gibbspost}{{4}{41}}
\citation{kirkpatrick1983optimization}
\citation{geman1984stochastic}
\citation{bishop2006pattern}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.4}Variational inference}{42}}
\newlabel{sec:variational}{{3.5.4}{42}}
\newlabel{eq:varassume}{{3.5.4}{42}}
\citation{bishop2006pattern}
\citation{murphy2012machine}
\newlabel{eq:varproperty}{{3.14}{43}}
\newlabel{eq:isingdot}{{3.15}{43}}
\citation{zhang1992mean}
\newlabel{eq:mf1}{{3.17}{44}}
\newlabel{eq:mf2}{{3.18}{44}}
\newlabel{eq:mf3}{{3.19}{44}}
\citation{greig1989exact}
\citation{boykov2001fast}
\citation{greig1989exact}
\citation{ford2010flows}
\citation{boykov2001fast}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.5}Graph cut optimization}{45}}
\newlabel{sec:graphcut}{{3.5.5}{45}}
\newlabel{eq:gcobj}{{3.20}{45}}
\citation{besag1975statistical}
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Parameter estimation}{47}}
\newlabel{sec:parest}{{3.6}{47}}
\citation{dempster1977maximum}
\citation{bishop2006pattern}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.1}Expectation maximization}{48}}
\newlabel{chap:em}{{3.6.1}{48}}
\newlabel{eq:qfunc}{{3.21}{48}}
\citation{wei1990monte}
\citation{feodor2000stochastic}
\citation{levine2001implementations}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.2}Monte Carlo EM}{49}}
\newlabel{c2sec:mcem}{{3.6.2}{49}}
\newlabel{eq:mcqfunc}{{3.22}{49}}
\citation{caffo2005ascent}
\citation{gelman1992inference}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.3}Convergence of MCEM}{51}}
\newlabel{eq:var}{{3.24}{51}}
\citation{robert2004monte}
\citation{johnson1996studying}
\citation{johnson1996studying}
\citation{gibbs2000bounding}
\citation{johnson1996studying}
\citation{gibbs2000bounding}
\citation{barbu2005generalizing}
\citation{huber2003bounding}
\citation{cooper1999mixing}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.4}Variational inference with EM a.k.a mean field theory}{54}}
\newlabel{chap:mathvar}{{3.6.4}{54}}
\citation{zhang1992mean}
\citation{johnson1996studying}
\citation{johnson1996studying}
\newlabel{eq:pseudoll}{{3.25}{55}}
\newlabel{eq:mfapp}{{3.26}{55}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces A graph model that represents the Markov chain.\relax }}{56}}
\newlabel{fig:mchain}{{3.1}{56}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Two graphical models represented by graphs. A graphical model representing a MRF can be either a regular grid or an general graph. For the regular grid, The node in blue color is conditional independent of the white node given it's adjacent neighbors, colored gray. For the general graph example, the two nodes in blue color are conditional independent given the remaining nodes. \relax }}{56}}
\newlabel{fig:mrf}{{3.2}{56}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces A simulation of MRF. When a new candidate $w$ is accepted to replace current $x_s$, we get a new set of variables $X^{m+1}$ that differs from the current variable $X$ at only $s$. The set of variable $X^m$ and $W^{m+1}$ is a sample of a Markov chain, since $X^{m+1}$ depends only on the previous $X^m$. Upon convergence, $X$ will be a sample from the target distribution $P(X)$. \relax }}{56}}
\newlabel{fig:imagechain}{{3.3}{56}}
\newlabel{fig:beta0.8}{{3.4a}{57}}
\newlabel{sub@fig:beta0.8}{{a}{57}}
\newlabel{fig:beta0.88}{{3.4b}{57}}
\newlabel{sub@fig:beta0.88}{{b}{57}}
\newlabel{fig:beta1.0}{{3.4c}{57}}
\newlabel{sub@fig:beta1.0}{{c}{57}}
\newlabel{fig:beta1.5}{{3.4d}{57}}
\newlabel{sub@fig:beta1.5}{{d}{57}}
\newlabel{fig:beta2.0}{{3.4e}{57}}
\newlabel{sub@fig:beta2.0}{{e}{57}}
\newlabel{fig:beta0.88details}{{3.4f}{57}}
\newlabel{sub@fig:beta0.88details}{{f}{57}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Simulating Ising model with various values of $\beta $. For each simulation, the image is initialized with random states, and then scanned 1000 times. Notice when $\beta $ is small, the image is less spatially coherent. When $\beta $ is large, the image has more spatial coherent regions. \relax }}{57}}
\newlabel{fig:isingsim}{{3.4}{57}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Simulating Potts model of four states with various values of $\beta $. For all simulations, the image was initialized with random states, and then was scanned 1000 times. \relax }}{57}}
\newlabel{fig:pottssim}{{3.5}{57}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces Consecutive samples of Potts model with $\beta = 1.1$ using SW and Gibbs sampling. Both samplers initialize the sample image with all-zero values, have 100 burn-in sampling and then save three consecutive samples. Note for the SW samples, multiple voxel labels have been changed between the consecutive sample images. Such multiple updates speed up convergence. For Gibbs, the three sample images are similar due to the strong interactions (relatively large $\beta $) between the neighboring nodes. \relax }}{58}}
\newlabel{fig:mathsw}{{3.6}{58}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces A graphical representation of the hidden Markov model(HMM). $X$ is defined on a regular lattice graph and is given a MRF prior to represent our knowledge of the smoothness or piecewise constant. $Y$ is the observed data that is generated from the likelihood function given the hidden $X$.\relax }}{58}}
\newlabel{fig:hmm}{{3.7}{58}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces Simulated annealing samples one variable at a time. Unike coordinate descent that always moves in the gradient descent direction (blue color arrow), the SA algorithm updates the variable based on a certain probability, which depends on the difference of the function value of two configurations (red arrow).\relax }}{59}}
\newlabel{fig:annealing}{{3.8}{59}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces Graph cut segmentation. Each voxel is defined as a node on a graph. Neighboring voxels have edges between them with weights given by MRF. A source node $s$ and a sink node $t$ are added. All nodes have links to both sources and sink nodes with weights depend on the likelihood function (data term). Graph cut algorithms find a cut, i.e., a set of edges whose overall weights are minimized. In the figure, edges with solid lines are kept, and edges with dashed lines are removed after the cut. Red think links are the cut. Node is assigned to source or sink label if they are connected to either of them. \relax }}{59}}
\newlabel{fig:graphcuts}{{3.9}{59}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.10}{\ignorespaces Recovering noise image by graph cut. Top row from left to right: a) observed noised image, b) ground truth label map, c) recovered label map. Bottom d) histogram of the observed image intensity. Note the region in blue circle of the true map is misclassified. \relax }}{60}}
\newlabel{fig:gcexample}{{3.10}{60}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.11}{\ignorespaces A hidden Markov model with $X$ in MRF, and each $y_s$ is independent Gaussian given $x_s$. The parameters are black dots, the hidden variables are circles, and the observed data are grayed circles. The MRF structure on $X$ is not shown in this diagram. Instead a box is on $X$ and $Y$ to represent that there are $N$ such nodes. \relax }}{60}}
\newlabel{fig:paraest}{{3.11}{60}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.12}{\ignorespaces Coding scheme for parameter estimation. For four-neighbors system of two-dimensional image, the voxels are separated into four groups. The voxels in the same group are conditionally independent given other groups.\relax }}{60}}
\newlabel{fig:coding}{{3.12}{60}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.13}{\ignorespaces Percentile of coupling iterations for Ising model of size $64\times 64$. Top curve shows the 99\% and bottom shows the 95\% percentile from the distribution of the iterations needed for coupling, as a function of $\beta $ parameter. The percentiles are estimated using 1000 repetitions of Gibbs sampling initialized with all-white and all-black value. (adapted from Johnson\nobreakspace  {}\cite  {johnson1996studying}. \relax }}{61}}
\newlabel{fig:ising}{{3.13}{61}}
\@setckpt{math}{
\setcounter{page}{62}
\setcounter{equation}{26}
\setcounter{enumi}{0}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{3}
\setcounter{figure}{13}
\setcounter{table}{0}
\setcounter{oldchapter}{0}
\setcounter{oldtocdepth}{0}
\setcounter{thrm}{0}
\setcounter{parentequation}{0}
\setcounter{mytheorem}{0}
\setcounter{myproposition}{0}
\setcounter{mydefinition}{0}
\setcounter{mycorollary}{0}
\setcounter{myexample}{0}
\setcounter{myexercise}{0}
\setcounter{AlgoLine}{0}
\setcounter{algocfline}{4}
\setcounter{algocfproc}{4}
\setcounter{algocf}{4}
\setcounter{ALC@unique}{0}
\setcounter{ALC@line}{0}
\setcounter{ALC@rem}{0}
\setcounter{ALC@depth}{0}
\setcounter{ContinuedFloat}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{mydef}{3}
\setcounter{section}{6}
\setcounter{subsection}{4}
\setcounter{paragraph}{0}
\setcounter{subsubsection}{0}
}
