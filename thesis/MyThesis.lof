\contentsline {figure}{\numberline {1.1}{\ignorespaces The order of estimation of various methods in group analysis.\relax }}{6}
\contentsline {figure}{\numberline {2.1}{\ignorespaces Segmentation map of a rs-fMRI volume. The spectral coherence between 0.01 to 0.1 Hz is used for similarity between pairs of voxels. A spectral clustering method\nobreakspace {}\cite {von2007tutorial} is used for dimension reduction followed by a K-Means clustering. We choose 12 clusters and 10 slices on $z$ direction to save computation time. \relax }}{23}
\contentsline {figure}{\numberline {2.2}{\ignorespaces Using a graph to represent functional networks. A series of ROIs are chosen based on what questions asked. Tthe signal at each ROI is computed by averaging the BOLD signals of all voxels within the sphere. The edge of the graph is estimated using the similarity of the signals between the ROIs. \relax }}{23}
\contentsline {figure}{\numberline {2.3}{\ignorespaces Slice timing correction. The data at temporally adjacent slices are resampled and interpolated to obtain a data point at the same time with the reference slices.\relax }}{24}
\contentsline {figure}{\numberline {2.4}{\ignorespaces Spatial ICA versus temporal ICA for BOLD series of length $T$ at $N$ voxels. In spatial ICA, all the voxel intensity at a single time point is assumed to be a mixed signal of $P$ independent signals, and there are $T$ such mixed signals. The decomposition is in the form of $X = A\cdot S$, where A is the weight coefficient and each row of $S$ is the independent source signal. We are interested in the $S$ since each row is regarded as a functional component. In temporal ICA, the BOLD time series of each voxel is the mixed signals, and there are $N$ such mixed signals. The decomposition is $\mathaccentV {tilde}07EX = \mathaccentV {tilde}07EA \cdot \mathaccentV {tilde}07ES$, with $\mathaccentV {tilde}07EA$ the weights, and rows in $\mathaccentV {tilde}07ES$ the independent signal. Here we are interested in the columns of $\mathaccentV {tilde}07EA$ as they are regarded as representations of the functional networks. \relax }}{24}
\contentsline {figure}{\numberline {3.1}{\ignorespaces A graph model that represents the Markov chain.\relax }}{56}
\contentsline {figure}{\numberline {3.2}{\ignorespaces Two graphical models represented by graphs. A graphical model representing a MRF can be either a regular grid or an general graph. For the regular grid, The node in blue color is conditional independent of the white node given it's adjacent neighbors, colored gray. For the general graph example, the two nodes in blue color are conditional independent given the remaining nodes. \relax }}{56}
\contentsline {figure}{\numberline {3.3}{\ignorespaces A simulation of MRF. When a new candidate $w$ is accepted to replace current $x_s$, we get a new set of variables $X^{m+1}$ that differs from the current variable $X$ at only $s$. The set of variable $X^m$ and $W^{m+1}$ is a sample of a Markov chain, since $X^{m+1}$ depends only on the previous $X^m$. Upon convergence, $X$ will be a sample from the target distribution $P(X)$. \relax }}{56}
\contentsline {figure}{\numberline {3.4}{\ignorespaces Simulating Ising model with various values of $\beta $. For each simulation, the image is initialized with random states, and then scanned 1000 times. Notice when $\beta $ is small, the image is less spatially coherent. When $\beta $ is large, the image has more spatial coherent regions. \relax }}{57}
\contentsline {figure}{\numberline {3.5}{\ignorespaces Simulating Potts model of four states with various values of $\beta $. For all simulations, the image was initialized with random states, and then was scanned 1000 times. \relax }}{57}
\contentsline {figure}{\numberline {3.6}{\ignorespaces Consecutive samples of Potts model with $\beta = 1.1$ using SW and Gibbs sampling. Both samplers initialize the sample image with all-zero values, have 100 burn-in sampling and then save three consecutive samples. Note for the SW samples, multiple voxel labels have been changed between the consecutive sample images. Such multiple updates speed up convergence. For Gibbs, the three sample images are similar due to the strong interactions (relatively large $\beta $) between the neighboring nodes. \relax }}{58}
\contentsline {figure}{\numberline {3.7}{\ignorespaces A graphical representation of the hidden Markov model(HMM). $X$ is defined on a regular lattice graph and is given a MRF prior to represent our knowledge of the smoothness or piecewise constant. $Y$ is the observed data that is generated from the likelihood function given the hidden $X$.\relax }}{58}
\contentsline {figure}{\numberline {3.8}{\ignorespaces Simulated annealing samples one variable at a time. Unike coordinate descent that always moves in the gradient descent direction (blue color arrow), the SA algorithm updates the variable based on a certain probability, which depends on the difference of the function value of two configurations (red arrow).\relax }}{59}
\contentsline {figure}{\numberline {3.9}{\ignorespaces Graph cut segmentation. Each voxel is defined as a node on a graph. Neighboring voxels have edges between them with weights given by MRF. A source node $s$ and a sink node $t$ are added. All nodes have links to both sources and sink nodes with weights depend on the likelihood function (data term). Graph cut algorithms find a cut, i.e., a set of edges whose overall weights are minimized. In the figure, edges with solid lines are kept, and edges with dashed lines are removed after the cut. Red think links are the cut. Node is assigned to source or sink label if they are connected to either of them. \relax }}{59}
\contentsline {figure}{\numberline {3.10}{\ignorespaces Recovering noise image by graph cut. Top row from left to right: a) observed noised image, b) ground truth label map, c) recovered label map. Bottom d) histogram of the observed image intensity. Note the region in blue circle of the true map is misclassified. \relax }}{60}
\contentsline {figure}{\numberline {3.11}{\ignorespaces A hidden Markov model with $X$ in MRF, and each $y_s$ is independent Gaussian given $x_s$. The parameters are black dots, the hidden variables are circles, and the observed data are grayed circles. The MRF structure on $X$ is not shown in this diagram. Instead a box is on $X$ and $Y$ to represent that there are $N$ such nodes. \relax }}{60}
\contentsline {figure}{\numberline {3.12}{\ignorespaces Coding scheme for parameter estimation. For four-neighbors system of two-dimensional image, the voxels are separated into four groups. The voxels in the same group are conditionally independent given other groups.\relax }}{60}
\contentsline {figure}{\numberline {3.13}{\ignorespaces Percentile of coupling iterations for Ising model of size $64\times 64$. Top curve shows the 99\% and bottom shows the 95\% percentile from the distribution of the iterations needed for coupling, as a function of $\beta $ parameter. The percentiles are estimated using 1000 repetitions of Gibbs sampling initialized with all-white and all-black value. (adapted from Johnson\nobreakspace {}\cite {johnson1996studying}. \relax }}{61}
\contentsline {figure}{\numberline {4.1}{\ignorespaces An example of correlation map with a seed in the default model network on the rs-fMRI data set.\relax }}{72}
\contentsline {figure}{\numberline {4.2}{\ignorespaces MRF prior of the connectivity variables. Each node of the graph represents a pairwise connectivity variable between voxel $i$ and $j$. An edge is added between two nodes $x_{ij}$ and $x_{ik}$ if $k$ is the neighbor of voxel $j$. The graph where the MRF is defined is twice the dimensions of the original image domain, i.e., six dimensions. Given the hidden variable $X$, the observed sample correlation values are assumed to be generated from a Gaussian distribution with unknown parameter $\mathcal {N}(y_{ij} | x_{ij}; \mu , \sigma ^2)$. \relax }}{72}
\contentsline {figure}{\numberline {4.3}{\ignorespaces Ideally the update of each voxel is independent of other voxels in order to be used on the GPU. In our Gibbs sampling, although the sampling of each voxel depends on its neighbors, the order of the voxels being updated does not matter. Upon convergence, the image will be a sample of the target Gibbs distribution. However, numerically, the sampling tends to be stuck in this local minimum of checkerboard image. At the current state, each voxel has a neighbor with a different state, and the sampling flips the color of all voxels in the next stage.\relax }}{73}
\contentsline {figure}{\numberline {4.4}{\ignorespaces Test on synthetic data.}}{73}
\contentsline {figure}{\numberline {4.5}{\ignorespaces Threshold correlation mapand Posterior Connectivity map between seed voxel and the current slice, overlaid to T2 image. (a) Subject 1: correlation without smoothing. (b) Subject 1: correlation with smoothing. (c) Subject 1: posterior estimated from MRF. (d) Subject 2: correlation without smoothing. (e) Correlation without smoothing. (f) Posterior estimated from MRF. \relax }}{74}
\contentsline {figure}{\numberline {4.6}{\ignorespaces Correlation map and posterior connectivity map between seed voxel and slice containing the seed. (a) Subject 1: correlation without smoothing. (b) Subject 1: correlation with smoothing. (c) Subject 1: posterior estimated from MRF. (d) Subject 2: correlation without smoothing. (e) Correlation without smoothing. (f) Posterior estimated from MRF. \relax }}{75}
\contentsline {figure}{\numberline {5.1}{\ignorespaces Data points with Von Mises-Fisher distribution.\relax }}{86}
\contentsline {figure}{\numberline {5.2}{\ignorespaces A generative model of the functional network. The network variable $X$ is a multivariate variable defined on a MRF. Given $X$, $Y$ is seen as being generated from a vMF distribution whose parameter $\mu $ and $\kappa $ are functions of $X$.\relax }}{86}
\contentsline {figure}{\numberline {5.3}{\ignorespaces Synthetic example. (a) True labels, (b) First time point of observed time series, (c) Time series plot on sphere, (d) label map estimated by mode-approximation, and label map estimated by MCEM.\relax }}{87}
\contentsline {figure}{\numberline {5.4}{\ignorespaces Functional networks detected by the proposed method for 3 subjects overlaid on their T1 images. The clusters are the visual (cyan), motor (green), executive control (blue), salience (magenta), dorsal attention (yellow), and default mode (red) networks.\relax }}{87}
\contentsline {figure}{\numberline {5.5}{\ignorespaces Comparison of the overlap of the label maps estimated by our MCEM approach, group ICA and single subject ICA on 16 subjects. Color map ranges from 8 (red) 16 (yellow). (a) MDN, (b) motor, (c) visual, (d) attentive.\relax }}{88}
\contentsline {figure}{\numberline {6.1}{\ignorespaces We define a MRF on a graph that includes the voxels of all subject maps as well as the group map. The set of edges includes the between-level links with weight $\alpha $, and within-subject links with weight $\beta $. The square box on the subject level and time courses repeats $J$ times the nodes in the square, representing all the subjects. Only the central voxels connection is shown for the between-level links, whereas in practice the links exist on all other voxels. The BOLD signal variables are shaded, meaning they are set to the observed value.\relax }}{110}
\contentsline {figure}{\numberline {6.2}{\ignorespaces An alternative representation of the graphical model of the HMRF. A regular MRF is defined on the network variables within subject, and within group label maps. Then between-level links are added between the group voxel and each subject voxel at the same anatomical location. The added edges, together with the original edges, consist of a new graph which integrates two levels of variables. \relax }}{110}
\contentsline {figure}{\numberline {6.3}{\ignorespaces Gibbs sampling schedule on a high level view. The sampling scan of all voxels in the group before updating each subject. This schedule repeats until convergence.\relax }}{111}
\contentsline {figure}{\numberline {6.4}{\ignorespaces Gibbs sampling iterates between group and subjects. On the voxel-level, the sampler draws samples of one voxel given its neighbors that includes both with-subject and between-level neighbors. \relax }}{111}
\contentsline {figure}{\numberline {6.5}{\ignorespaces The estimated group and subject functional network label maps from various methods, as well as the ground truth maps. Only two are shown among the 25 subjects. \relax }}{111}
\contentsline {figure}{\numberline {6.6}{\ignorespaces Box-and-whiskers plots of the estimation accuracies of all methods for three levels of spatial smoothing. The accuracies of subject labels are across all subjects and MC samples. The group map accuracies are across all MC samples. The upper and lower ``hinges'' correspond to the $25$th and $75$th percentiles. The asterisk on top of each box indicates the p-value of the standard two-tailed T test between HMRF and the corresponding method. No asterisk: significant $p > 0.05$; $*$: significant at $p < 0.05$; $**$: significant at $p < 0.01$; ${***}$: significant at $p < 0.001$. The group map is not applicable to HMRF-B due to its lack of between-level links.\relax }}{112}
\contentsline {figure}{\numberline {6.7}{\ignorespaces Box-and-whiskers plots of the RI value between each pair of sessions over the all subjects' label map. The bottom and top of the boxes are the $25$th and $75$th percentile, and the whiskers extend to the whole range of the data except the outliers.\relax }}{113}
\contentsline {figure}{\numberline {6.8}{\ignorespaces The intersession variance maps for three segmentation methods. The variance maps are obtained for each subject, averaged across subjects, and finally normalized to [0, 1]. A few voxels with intensity above 0.8 are rendered the same as those with intensity 0.8. This single map covers all seven functional networks, and we selectively show the slices corresponding to the three major networks. The image left is the subject's left, and we use the same convention in the following figures. \relax }}{113}
\contentsline {figure}{\numberline {6.9}{\ignorespaces The group level's mean functional networks estimated from all bootstrapped data by three segmentation methods. The binary map of each network is averaged over all bootstrap samples. The average intensity ranges from 0 to 1.\relax }}{114}
\contentsline {figure}{\numberline {6.10}{\ignorespaces The group variance map estimated from all bootstrap data by the three segmentation methods. The variance ranges from 0 to 0.25. \relax }}{115}
\contentsline {figure}{\numberline {6.11}{\ignorespaces The three subjects' average network label maps estimated from all bootstrap samples. One representative slice is shown for each of the seven networks for each subject (row) and each method (column), excluding brain stem component. The average values range from 0 to 1. \relax }}{116}
\contentsline {figure}{\numberline {6.12}{\ignorespaces The subjects' variance maps estimated from all bootstrap samples. The maps are averaged across all subjects, and their values range from 0 to 0.25. The color map is in [0, 0.15] since most of the variance values fall into this range.\relax }}{117}
\contentsline {figure}{\numberline {6.13}{\ignorespaces Estimation of parameter $\alpha $ with the average predictive distributions using the leave-one-out cross-validation. We use the data from only the first session of the NYU-TRT dataset but find similar patterns in the other two sessions. $\alpha $ are sampled between 0.15 and 0.5, with interval 0.05.\relax }}{117}
